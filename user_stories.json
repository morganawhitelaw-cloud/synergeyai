[
  {
    "ID": "US-001",
    "Epic": "Session Management",
    "Tool_Process": "Project Creation (Landing Page and authentication)",
    "Feature": "Login via Capital Edge SSO",
    "User_Story": "As a user, I want to log in using Capital Edge Single Sign-On\nSo that I can securely access the application without separate credentials",
    "Description": null,
    "Scenario": "\nScenario 1: Display SSO login option\n    Given the user is on the application login page\n    When the page loads\n    Then the page shows a button \"Sign in with Capital Edge SSO\"\n\n  Scenario 2: Redirect to Capital Edge SSO login page\n    Given the user clicks \"Sign in with Capital Edge SSO\"\n    When the system processes the request\n    Then the user is redirected to Capital Edge SSO login page over HTTPS\n    And the redirect URL contains a valid client ID and state parameter\n",
    "Acceptance_Criteria": "1. Login page displays an option for “Sign in with Capital Edge SSO”.\n2. Clicking the option redirects to Capital Edge SSO login page.\n3. Redirection uses HTTPS and secure OAuth/OpenID Connect protocol.\n4. System maintains session state during redirection.\n5. Page loads successfully and shows the SSO button.",
    "Sprint": 1.0,
    "Effort_Point": 1.0,
    "Effort_Required": 8.0,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": "In Progress",
    "Comments": "SSO Implemented. Need to check if it is compatible with cap Edge"
  },
  {
    "ID": "US-002",
    "Epic": "Session Management",
    "Tool_Process": "Project Creation (Landing Page and authentication)",
    "Feature": "Validate SSO token and create session",
    "User_Story": "As a user, I want the system to validate my identity via Capital Edge SSO, So that I can securely access the application after successful authentication",
    "Description": null,
    "Scenario": "\nScenario 1: Successful authentication\n    Given the user has logged in via Capital Edge SSO\n    And the SSO provider returns a valid token\n    When the application validates the token\n    Then the user session is created\n    And the user is redirected to the dashboard\n\n  Scenario 2: Invalid token\n    Given the SSO provider returns an invalid or expired token\n    When the application attempts validation\n    Then the system displays \"Authentication failed\"\n",
    "Acceptance_Criteria": "1. After successful login at Capital Edge, user is redirected back to the application.\n2. Application receives and validates the SSO token.\n3. Session is created with user details (name, email, roles).\n4. Failed token validation results in error message and no session.",
    "Sprint": 1.0,
    "Effort_Point": 1.0,
    "Effort_Required": 8.0,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": "In Progress",
    "Comments": "SSO Implemented. Need to check if it is compatible with cap Edge"
  },
  {
    "ID": "US-003",
    "Epic": "Session Management",
    "Tool_Process": "Project Creation (Landing Page and authentication)",
    "Feature": "Authentication error handling",
    "User_Story": "As a user, I want clear error messages when login fails\nSo that I understand what went wrong and can retry",
    "Description": null,
    "Scenario": "\nScenario 1: Invalid credentials at SSO\n    Given the user enters incorrect credentials on Capital Edge SSO page\n    When the SSO provider rejects the login\n    Then the application displays \"Login failed. Please try again.\"\n\n  Scenario 2: Expired token\n    Given the user attempts to return to the application after token expiry\n    When the application validates the token\n    Then the system displays \"Session expired. Please log in again.\"",
    "Acceptance_Criteria": "1. Display error for invalid credentials at SSO provider.\n2. Display error for expired session or token.\n3. Log all authentication failures for audit.",
    "Sprint": 1.0,
    "Effort_Point": 1.0,
    "Effort_Required": 8.0,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": "In Progress",
    "Comments": "SSO Implemented. Need to check if it is compatible with cap Edge"
  },
  {
    "ID": "US-004",
    "Epic": "Session Management",
    "Tool_Process": "Project Creation (Landing Page and authentication)",
    "Feature": "Apply Session watermarking",
    "User_Story": "As a user, I want the tool to display a watermark with project name, timestamp, session ID, and user\nSo that I can identify session context and prevent misuse",
    "Description": null,
    "Scenario": "\nScenario 1: Display watermark with session details\n    Given the user is logged in and opens the tool\n    When the session is active\n    Then the watermark shows:\n      | Project Name | Timestamp | Session ID | User |\n    And the watermark is visible on all screens\n\nScenario 2: Watermark displays correctly with special characters in project name\n  Given the project name is \"R&D@2025\"\n  And the user is logged in\n  When the session starts\n  Then the watermark shows \"R&D@2025 | Timestamp | SessionID | User\"\n\nScenario 3: Watermark persists across multiple tabs\n  Given the user opens two tabs of the tool\n  When the session is active\n  Then both tabs display the correct watermark for the same session\n",
    "Acceptance_Criteria": "1. Watermark appears on every page of the tool.\n2. Watermark includes: project name, timestamp (UTC), session ID, and username.\n3. Watermark updates if session changes.\n4. Watermark is non-removable and semi-transparent.",
    "Sprint": 1.0,
    "Effort_Point": 2.0,
    "Effort_Required": 16.0,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": "Completed",
    "Comments": "Testing pending. will be completed once the Reddis and Open AI services are available"
  },
  {
    "ID": "US-005",
    "Epic": "Session Management",
    "Tool_Process": "Project Creation (Landing Page and authentication)",
    "Feature": "Activity logging",
    "User_Story": "As a system, I want to log user actions and analysis selections\nSo that audit and reporting are accurate",
    "Description": null,
    "Scenario": "\nScenario 1: Log user actions and analysis selections\n    Given the user selects \"HC\" and \"Non HC\" analyses\n    When the user works for 15 minutes\n    Then the log entry includes:\n      | User | Session ID | Analyses | Duration |\n      | Neil | S12345 | HC, Non HC | 15 mins |\n",
    "Acceptance_Criteria": "1. Log includes: user ID, timestamp, selected analyses (HC, non-HC), duration of usage.\n2. Logs stored securely and tamper-proof.\n3. Logs capture session start, end, and inactivity periods.",
    "Sprint": 1.0,
    "Effort_Point": 2.0,
    "Effort_Required": 16.0,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": "Completed",
    "Comments": "Testing pending. will be completed once the Reddis and Open AI services are available"
  },
  {
    "ID": "US-006",
    "Epic": "Session Management",
    "Tool_Process": "Project Creation (Landing Page and authentication)",
    "Feature": "Session timeout",
    "User_Story": "As a user, I want the system to warn me before session timeout and allow extension\nSo that I don’t lose unsaved work",
    "Description": null,
    "Scenario": "\nScenario 1: Warn user before timeout\n    Given the user is inactive for 29 minutes\n    When the system detects inactivity\n    Then a dialog appears with message \"Session will expire in 1 minute\"\n    And options \"Continue\" and \"Logout\"\n\n  Scenario 2: Extend session\n    Given the warning dialog is displayed\n    When the user clicks \"Continue\"\n    Then the session timer resets to 30 minutes\n\n  Scenario 3: Timeout after inactivity\n    Given the user does not respond to the warning\n    When 1 minute passes\n    Then the session ends\n    And unsaved work is lost\n\nScenario 4: User interacts just before timeout\n  Given the user is inactive for 29 minutes and 59 seconds\n  When the user clicks anywhere in the tool\n  Then the session timer resets to 30 minutes\n  And no warning dialog is shown\n\nScenario 5: User clicks Continue after timeout\n  Given the warning dialog was displayed\n  And the session expired while the dialog was open\n  When the user clicks \"Continue\"\n  Then the system displays \"Session expired. Please log in again\"\n  And redirects to the login page\n\nScenario 6: Multiple tabs timeout consistency\n  Given the user has two tabs open in the same session\n  When the session expires in one tab\n  Then the other tab also shows \"Session expired\" immediately\n",
    "Acceptance_Criteria": "1. Session times out after 30 minutes of inactivity.\n2. Warning dialog appears at 29 minutes with option to “Continue”.\n3. Clicking “Continue” resets timer to 30 minutes.\n4. If user does nothing, session ends and unsaved work is lost.",
    "Sprint": 1.0,
    "Effort_Point": 2.0,
    "Effort_Required": 16.0,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": "Completed",
    "Comments": "Need to check if all the scenarios are implemented (Adarsha nd Aditya to get back)"
  },
  {
    "ID": "US-007",
    "Epic": "Project Creation",
    "Tool_Process": "Project Creation (Project Creation Wizard)",
    "Feature": "Create a new project from UI",
    "User_Story": "As a user, I want to open the application user interface,\nSo that I can access the functionality to create a new project.",
    "Description": null,
    "Scenario": "Scenario 1: Successfully open the UI\nGiven the application is installed and accessible\nWhen the user launches the application\nThen the application UI should load successfully\nAnd the home screen should display the main navigation options\n\nScenario 2: Application fails to open\nGiven the application is installed\nWhen the user launches the application\nAnd the application encounters an error\nThen an error message should be displayed\nAnd the user should have an option to retry or exit\n\n",
    "Acceptance_Criteria": "1. The application launches without errors.\n2. The UI loads within 5 seconds.\n3. The home screen displays:\n       A visible navigation menu.\n       A “New Project” button or link.\n4. If the application fails to load, an error message is displayed with retry option.\n5. Error message includes:\n      Clear description of the issue.\n      Retry and Exit buttons.\n6. Application logs the error for troubleshooting.",
    "Sprint": 1.0,
    "Effort_Point": 1.0,
    "Effort_Required": 8.0,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": "Completed",
    "Comments": "Need to check if all the scenarios are implemented (Adarsha nd Aditya to get back)"
  },
  {
    "ID": "US-008",
    "Epic": "Project Creation",
    "Tool_Process": "Project Creation (Project Creation Wizard)",
    "Feature": "Select \"New Project\"",
    "User_Story": "As a user, I want to select the ‘New Project’ option from the UI,\nSo that I can start creating a new project.",
    "Description": null,
    "Scenario": "Scenario 1: Successfully select ‘New Project’\nGiven the application UI is open\nAnd the user is on the home screen\nWhen the user clicks on “New Project”\nThen the system should navigate to the project creation screen\nAnd display fields for project details\n\nScenario 2: ‘New Project’ option is disabled\nGiven the application UI is open\nAnd the user is on the home screen\nWhen the user tries to click “New Project”\nAnd the option is disabled due to insufficient permissions\nThen the system should display an error message\nAnd suggest contacting the administrator\n\nScenario 3: System error while opening ‘New Project’\nGiven the application UI is open\nAnd the user clicks “New Project”\nWhen the system encounters an error loading the project creation screen\nThen an error message should be displayed\nAnd the user should have an option to retry or return to home",
    "Acceptance_Criteria": "1. “New Project” button is visible and clickable.\n2. Clicking the button navigates to the project creation screen.\n3. Project creation screen includes:\n        Project Name field (mandatory).\n        Description field (optional).\n        Save and Cancel buttons.\n4. Disabled state is visually distinct (greyed out).\n5. Error message explains lack of permissions.\n6. No navigation occurs.\n7. Error message includes:\n        Clear description of the issue.\n        Retry and Home buttons.\n8. System logs the error.",
    "Sprint": 1.0,
    "Effort_Point": 4.0,
    "Effort_Required": 32.0,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": "Completed",
    "Comments": "Need to check if all the scenarios are implemented (Adarsha nd Aditya to get back)"
  },
  {
    "ID": "US-009",
    "Epic": "Project Creation",
    "Tool_Process": "Project Creation (Project Creation Wizard)",
    "Feature": "Enter project details",
    "User_Story": "As a user, I want to see all required fields for project details based on selected analysis type, So that I can provide accurate information for the project.",
    "Description": null,
    "Scenario": "Scenario 1: Display all mandatory fields\nGiven the user has selected \"New Project\"\nWhen the project details screen loads\nThen the form should display all required fields\nAnd fields should be grouped logically (General Info, Buyer Details, Target Details, Analysis Options)",
    "Acceptance_Criteria": "1. Sections:\n               General Info: Project Name, Description, Legal Hold, Engagement Partner, CIC Clause Confirmations.\n               Buyer Details: Name, Public/Private, Financials, Region, FTEs, Shared Services.\n               Target Details: Same as Buyer.\n               Analysis Toggles: HC, Non-HC, Real Estate.\n               SpendCube-specific fields only appear if Non-HC analysis is toggled.\n               Census-specific fields only appear if HC analysis is toggled\n2. All mandatory fields marked with an asterisk.\n3. Validation messages for missing required fields.\n\n",
    "Sprint": 1.0,
    "Effort_Point": 10.0,
    "Effort_Required": 80.0,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": "Yet to start",
    "Comments": "TO DO: Configure SQL DB to SQL API (Refer to other projects in capital edge and follow similar implementation parocess)"
  },
  {
    "ID": "US-010",
    "Epic": "Project Creation",
    "Tool_Process": "Project Creation (Project Creation Wizard)",
    "Feature": "Enter project details",
    "User_Story": "As a user, I want to toggle analysis types (HC, Non-HC, Real Estate),\nSo that only relevant fields appear.",
    "Description": null,
    "Scenario": "Scenario 1: HC analysis selected\nGiven the user is on the project details screen\nWhen the user toggles HC analysis ON\nThen fields required for census should appear\nAnd fields for SpendCube should remain hidden\n\nScenario 2: Non-HC analysis selected\nGiven the user is on the project details screen\nWhen the user toggles Non-HC analysis ON\nThen fields required for SpendCube should appear\nAnd fields for census should remain hidden\n",
    "Acceptance_Criteria": "1. HC toggle ON → Show:\n            Region of operations.\n            No. of FTEs.\n            Shared services presence.\n2. HC toggle OFF → Hide these fields.\n3. Non-HC toggle ON → Show:\n            BU.\n            Type of spend.\n             Peers.\n              Min/Max revenue.\n4. Non-HC toggle OFF → Hide these fields.\n\n",
    "Sprint": 1.0,
    "Effort_Point": 2.0,
    "Effort_Required": 16.0,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": "Need to check if all the scenarios are implemented (Adarsh and Aditya to get back)"
  },
  {
    "ID": "US-011",
    "Epic": "Project Creation",
    "Tool_Process": "Project Creation (Project Creation Wizard)",
    "Feature": "Currency Conversion",
    "User_Story": "As a user, I want to toggle currency conversion and select a currency,\nSo that financial data is converted correctly.",
    "Description": null,
    "Scenario": "Scenario 1: Currency conversion enabled\nGiven the user is on the project details screen\nWhen the user toggles currency conversion ON\nThen a dropdown to select target currency should appear\nAnd the user can choose from predefined currencies (USD, EUR, GBP, etc.)",
    "Acceptance_Criteria": "1. Toggle OFF → No currency dropdown.\n2. Toggle ON → Dropdown appears with at least 5 major currencies.\n3. Default currency = USD.\n4. Validation: User must select a currency if toggle is ON.",
    "Sprint": 1.0,
    "Effort_Point": 5.0,
    "Effort_Required": 40.0,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": "Yet to start",
    "Comments": "UI"
  },
  {
    "ID": "US-012",
    "Epic": "Project Creation",
    "Tool_Process": "Project Creation (Project Creation Wizard)",
    "Feature": "Auto-populate data from Cap IQ",
    "User_Story": "As a user, I want to auto-populate company details from CapIQ if available,\nSo that I save time and reduce manual entry.",
    "Description": null,
    "Scenario": "Scenario 1: Company found in CapIQ\nGiven the user enters Buyer/Target Name\nAnd the company exists in CapIQ\nWhen the system fetches data\nThen Buyer financials and sector details should auto-populate\nAnd user can edit these fields if needed\n\nScenario 2: Company not found in CapIQ\nGiven the user enters Buyer/Target Name\nAnd the company does not exist in CapIQ\nWhen the system attempts to fetch data\nThen the system should display a message “Company not found”\nAnd allow manual input for all fields",
    "Acceptance_Criteria": "1. Auto-populate:\n     Revenue, COGS, SG&A.\n     Sector/Sub-sector.\n2. Editable fields after auto-population.\n3. Display “Data fetched from CapIQ” message.\n4. Manual entry enabled for all fields if data not available in CapIQ.\n5. Clear message for missing data is data not available in CapIQ.",
    "Sprint": 1.0,
    "Effort_Point": 10.0,
    "Effort_Required": 80.0,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": "Completed",
    "Comments": "SQL Database/Databricks for CE platform to be provisioned. CapIQ (Size of the data Adarsh to get back)"
  },
  {
    "ID": "US-013",
    "Epic": "Project Creation",
    "Tool_Process": "Project Creation (Project Creation Wizard)",
    "Feature": "Deal type Auto-population",
    "User_Story": "As a user, I want the Deal Type to auto-populate based on predefined logic, So that I don’t have to manually select it.",
    "Description": null,
    "Scenario": "Scenario 1: Auto-populate deal type\nGiven the user has answered the deal-related questions\nWhen the system evaluates responses\nThen the Deal Type field should auto-populate as “Merger of Equals” or “Tuck-in/Acquisition”",
    "Acceptance_Criteria": "1. Logic documented and implemented.\n2. Field is read-only after auto-population.\n3. If logic fails, default to “Tuck-in/Acquisition” and display warning.",
    "Sprint": 1.0,
    "Effort_Point": 5.0,
    "Effort_Required": 40.0,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": "Completed",
    "Comments": null
  },
  {
    "ID": "US-014",
    "Epic": "Project Creation",
    "Tool_Process": "Project Creation (Project Creation Wizard)",
    "Feature": "Deal type Auto-population",
    "User_Story": "As a user, I want the Deal Type to be automatically determined based on revenue ratio and sector match,\nSo that I don’t have to manually select it and the classification is consistent.",
    "Description": "Business Logic\n\nIf Target Revenue ≥ 75% of Buyer Revenue AND Target Sector = Buyer Sector, then Deal Type = Merger of Equals.\nElse, Deal Type = Tuck-in/Acquisition.",
    "Scenario": "Scenario 1: Merger of Equals\nGiven the user has entered Buyer Revenue and Target Revenue\nAnd the user has entered Buyer Sector and Target Sector\nWhen Target Revenue is 75% or more of Buyer Revenue\nAnd Target Sector is the same as Buyer Sector\nThen the Deal Type should auto-populate as \"Merger of Equals\"\n\nScenario 2: Tuck-in/Acquisition\nGiven the user has entered Buyer Revenue and Target Revenue\nAnd the user has entered Buyer Sector and Target Sector\nWhen Target Revenue is less than 75% of Buyer Revenue\nOr Target Sector is different from Buyer Sector\nThen the Deal Type should auto-populate as \"Tuck-in/Acquisition\"\n\nScenario 3: Missing Data\nGiven the user has not entered Buyer Revenue or Target Revenue\nOr Buyer Sector or Target Sector\nWhen the system attempts to auto-populate Deal Type\nThen the system should display a warning message\nAnd default Deal Type to \"Tuck-in/Acquisition\"",
    "Acceptance_Criteria": "1. Calculation: Target Revenue ÷ Buyer Revenue ≥ 0.75.\n2. Sector comparison is case-insensitive.\n3. Deal Type field becomes read-only after auto-population.\n4. Display message: “Deal Type auto-populated based on revenue and sector.”\n5. Any failure in revenue ratio or sector match defaults to Tuck-in/Acquisition.\n6. Field remains read-only.\n7. Display message: “Deal Type auto-populated based on revenue and sector.”\n8. Warning message: “Insufficient data for Deal Type logic. Default applied.”\n9. Field remains editable if logic cannot be applied.",
    "Sprint": 1.0,
    "Effort_Point": 5.0,
    "Effort_Required": 40.0,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": "Scenario 1 and 2 -  completed, Scenario 3 to confirm",
    "Comments": null
  },
  {
    "ID": "US-015",
    "Epic": "Project Creation",
    "Tool_Process": "Project Creation (Confirm and Launch)",
    "Feature": "Summary Screen",
    "User_Story": "As a user,  I want to view a summary of all project details entered,  So that I can verify the information before proceeding.",
    "Description": null,
    "Scenario": "Scenario 1: Display all entered details\nGiven the user has completed the project details form\nWhen the user navigates to the Summary Screen\nThen the system should display all entered details grouped by sections\nAnd allow the user to review them\n\n",
    "Acceptance_Criteria": "1. Sections displayed:\n         General Info (Project Name, Description, Legal      Hold, Engagement Partner, CIC Clause).\n         Buyer Details.\n         Target Details.\n          Analysis Toggles and related fields.\n2. All fields show the latest saved values.\n3. Read-only mode for summary (no editing here).\n4. Option to go back and edit details.",
    "Sprint": 1.0,
    "Effort_Point": 5.0,
    "Effort_Required": 40.0,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": "Yet to start",
    "Comments": "UI (FE)"
  },
  {
    "ID": "US-016",
    "Epic": "Project Creation",
    "Tool_Process": "Project Creation (Confirm and Launch)",
    "Feature": "Summary Screen Actions: display summary screen",
    "User_Story": "As a user, I want to view all entered details and required next steps, So that I can verify and decide the next action.",
    "Description": null,
    "Scenario": "\nScenario: Summary screen loads successfully\nGiven the user has completed the project details form\nWhen the user navigates to the Summary Screen\nThen the system should display:\n  - All entered details grouped by sections\n  - Required next steps based on selected analyses\n  - Action buttons: Save & Continue, Save as Draft, Cancel",
    "Acceptance_Criteria": "1. Summary screen shows:\n         General Info, Buyer Details, Target Details, Analysis Toggles.\n         Required files list (Census Data, PO Data, Facility Logs based on toggles).\n2. Action buttons visible and enabled:\n         Save & Continue → Saves data and moves to next step.\n         Save as Draft → Saves data without proceeding.\n         Cancel → Discards unsaved changes and returns to home screen.\n3. Confirmation prompt for Cancel action.",
    "Sprint": 1.0,
    "Effort_Point": 5.0,
    "Effort_Required": 40.0,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": "Yet to start",
    "Comments": null
  },
  {
    "ID": "US-017",
    "Epic": "Project Creation",
    "Tool_Process": "Project Creation (Confirm and Launch)",
    "Feature": "Summary Screen Actions: Save and Continue",
    "User_Story": "As a user, I want to save all entered details and proceed to the next step, So that I can complete the workflow without losing data.",
    "Description": null,
    "Scenario": "Scenario: Save & Continue clicked\nGiven the user is on the Summary Screen\nWhen the user clicks \"Save & Continue\"\nThen the system should validate all mandatory fields and uploads\nAnd save the project details\nAnd navigate to the next step",
    "Acceptance_Criteria": "1. Validation:\n           All required fields completed.\n          All mandatory files uploaded.\n2. If validation fails:\n          Display error message and highlight missing items.\n3. If validation passes:\n          Save data to database.\n          Navigate to next step (e.g., confirmation or dashboard).",
    "Sprint": 1.0,
    "Effort_Point": 2.0,
    "Effort_Required": 16.0,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": "Yet to start",
    "Comments": null
  },
  {
    "ID": "US-018",
    "Epic": "Project Creation",
    "Tool_Process": "Project Creation (Confirm and Launch)",
    "Feature": "Summary Screen Actions: Save as draft",
    "User_Story": "As a user, I want to save the project as a draft without completing uploads, So that I can resume later.",
    "Description": null,
    "Scenario": "Scenario: Save as Draft clicked\nGiven the user is on the Summary Screen\nWhen the user clicks \"Save as Draft\"\nThen the system should save all entered details\nAnd mark the project status as \"Draft\"\nAnd return to the dashboard",
    "Acceptance_Criteria": "1. No validation for mandatory uploads.\n2. Save all entered data.\n3. Status = Draft.\n4. Display success message: “Project saved as draft.”",
    "Sprint": 1.0,
    "Effort_Point": 2.0,
    "Effort_Required": 16.0,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": "Yet to start",
    "Comments": null
  },
  {
    "ID": "US-019",
    "Epic": "Project Creation",
    "Tool_Process": "Project Creation (Confirm and Launch)",
    "Feature": "Summary Screen Actions: Cancel",
    "User_Story": "As a user, I want to cancel the process and return to the home screen, So that I can exit without saving changes.",
    "Description": null,
    "Scenario": "Scenario: Cancel clicked\nGiven the user is on the Summary Screen\nWhen the user clicks \"Cancel\"\nThen the system should display a confirmation prompt\nAnd if the user confirms\nThen discard unsaved changes\nAnd return to the home screen",
    "Acceptance_Criteria": "1. Confirmation prompt: “Are you sure you want to cancel? Unsaved changes will be lost.”\n2. If confirmed → Discard changes and navigate home.\n3. If declined → Stay on Summary Screen.",
    "Sprint": 1.0,
    "Effort_Point": 2.0,
    "Effort_Required": 16.0,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": "Yet to start",
    "Comments": null
  },
  {
    "ID": "US-020",
    "Epic": "Project Creation",
    "Tool_Process": "Project Creation (Section Navigation)",
    "Feature": "Navigation between Analysis Tab: Provide Navigation Links",
    "User_Story": "As a user, I want to see navigation links for Census, Spend analyses in order, So that I can move through the workflow based on selected analyses and data availability.",
    "Description": null,
    "Scenario": "Scenario 1: Display navigation links\nGiven the user is on the Summary Screen\nWhen the system loads navigation options\nThen the system should display links for:\n  - Tab 2a: Census Analysis\n  - Tab 2b: Spend Analysis\n   - Tab 3: Outputs\nAnd the links should be ordered as Census → Spend → Real Estate → Outputs",
    "Acceptance_Criteria": "1. Links visible in correct order.\n2. Links enabled only if analysis type was selected.\n3. Disabled links show tooltip: “selection missing.”",
    "Sprint": 1.0,
    "Effort_Point": 4.0,
    "Effort_Required": 32.0,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": "Yet to start",
    "Comments": null
  },
  {
    "ID": "US-021",
    "Epic": "Project Creation",
    "Tool_Process": "Project Creation (Section Navigation)",
    "Feature": "Navigation between Analysis Tab: Navigate to Census Analysis",
    "User_Story": "As a user, I want to move to Census Analysis if HC analysis is selected, So that I can perform HC-related analysis.",
    "Description": null,
    "Scenario": "Scenario 1: HC analysis applicable\nGiven the user selected HC analysis\nWhen the user clicks \"Census Analysis\"\nThen the system should navigate to Tab 2a\n\nScenario 2: HC analysis not applicable\nGiven the user did not select HC analysis\nWhen the user clicks \"Census Analysis\"\nThen the system should skip Tab 2a\nAnd move to the next applicable step",
    "Acceptance_Criteria": "1. Navigation occurs only if HC toggle = ON \n2. If condition not met: Display error: “HC analysis not applicable.”\n3. Do not navigate.\n4. System auto-skips to Spend Analysis if HC not selected.\n5. No error message shown; smooth transition.",
    "Sprint": 1.0,
    "Effort_Point": 3.0,
    "Effort_Required": 24.0,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": "Yet to start",
    "Comments": null
  },
  {
    "ID": "US-022",
    "Epic": "Project Creation",
    "Tool_Process": "Project Creation (Section Navigation)",
    "Feature": "Navigation between Analysis Tab: Navigate to Spend Analysis",
    "User_Story": "As a user, I want to move to Spend Analysis if Spend analysis is selected, So that I can perform Non-HC-related analysis.",
    "Description": null,
    "Scenario": "Scenario: Given the user selected Non-HC analysis\nWhen the user clicks \"Spend Analysis\"\nThen navigate to Tab 2b\nElse skip to next applicable step",
    "Acceptance_Criteria": "1. Navigation occurs only if non-HC toggle = ON \n2. If condition not met: Display error: “non-HC analysis not applicable.”\n3. Do not navigate.\n4. System auto-skips to Consolidated outputAnalysis if HC not selected.\n5. No error message shown; smooth transition.",
    "Sprint": 1.0,
    "Effort_Point": 3.0,
    "Effort_Required": 24.0,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": "Yet to start",
    "Comments": null
  },
  {
    "ID": "US-023",
    "Epic": "Project Creation",
    "Tool_Process": "Project Creation (Section Navigation)",
    "Feature": "Navigation between Analysis Tab: Navigate to Consolidated output",
    "User_Story": "As a user, I want to move to Consolidated output if all the applicable analysis is completed, So that I can get the consolidated output.",
    "Description": null,
    "Scenario": "Scenario: Given the user has completed all applicable analyses\nWhen the user clicks \"Outputs\"\nThen the system should display results from all performed analyses",
    "Acceptance_Criteria": "1. Tab 3 always accessible after completing or skipping previous steps.\n2. Displays consolidated outputs.",
    "Sprint": 1.0,
    "Effort_Point": 3.0,
    "Effort_Required": 24.0,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": "Yet to start",
    "Comments": null
  },
  {
    "ID": "US-024",
    "Epic": "HC Analysis ",
    "Tool_Process": "Census",
    "Feature": "File Upload for HC Analysis",
    "User_Story": "As a user, I want to upload Census data files for Buyer and Target with clear instructions and validation, So that I can perform HC analysis accurately.",
    "Description": null,
    "Scenario": "Scenario 1: Display upload prompt for Census\nGiven HC analysis is selected\nWhen the Census Analysis tab loads\nThen the system should prompt the user to upload Buyer Census and Target Census files\nAnd display:\n  - Instructions for required fields\n  - Sample template link\n  - Note that files must be Excel format\n\nScenario 2: Validate Census file\nGiven the user uploads a Census file\nWhen the system processes the file\nThen the system should check for missing mandatory fields in any row\nAnd if missing fields exist\nThen mark status as \"Needs Review\"\nAnd display a message prompting the user to update or override\n\nScenario 3: Merger vs Tuck-in logic for Census\nGiven the deal type is Merger of Equals\nWhen uploading Census files\nThen both Buyer and Target files are mandatory\nElse if deal type is Tuck-in/Acquisition\nThen Target file is mandatory and Buyer file is optional",
    "Acceptance_Criteria": "1. Required fields: Employee ID, Job Title, Job Description, Function, Sub-Function, Cost Center, Address, Job Office City, Fully Loaded Cost, Currency, Employee Type.\n2. Instructions include:\n             “Ensure all mandatory fields are filled.”\n              “File must be in .xls or .xlsx format.”\n3. Sample template downloadable.\n4. Status indicators: Missing, Uploaded, Needs Review.\n5. Missing mandatory fields → Needs Review.\n6. Display message: “Some rows have missing data. Update file or override to proceed.”\n7. Override option available with confirmation.\n8. Validation enforced based on deal type.\n9. Error message if mandatory file missing: “You must upload Target Census.xlsx before proceeding. Click here for a template.”\n10. Merger of Equals → Both Buyer and Target data mandatory.\n11. Tuck-in/Acquisition → Target data mandatory; Buyer optional.\n12. If required entity data missing → Error message:\n“Target data is mandatory for Tuck-in/Acquisition. Please upload complete file.”\n\n",
    "Sprint": 2.0,
    "Effort_Point": 10.0,
    "Effort_Required": 80.0,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-025",
    "Epic": "HC Analysis",
    "Tool_Process": "Census",
    "Feature": "File Upload for analysis: Upload Buyer Census file",
    "User_Story": "As a user, I want to upload the Buyer census file So that I can provide Buyer data for HC analysis.",
    "Description": null,
    "Scenario": "Scenario 1: Successfully upload Buyer Census file\n    Given HC analysis is selected\n    When the user uploads Buyer Census file in .xlsx format\n    Then the system should accept the file\n    And display status as \"Uploaded\"\n\n  Scenario 2: File missing mandatory columns\n    Given the user uploads Buyer Census file\n    When mandatory columns are missing\n    Then the system should reject the file\n    And display error message \"Missing required columns: Employee ID, Job Title, etc.\"\n\n  Scenario 3: Large file upload with chunking\n    Given the Buyer Census file has more than 100k rows\n    When the system processes the upload\n    Then the file should be split into chunks\n    And each chunk verified using checksum\n\nScenario 4: Missing entity data for deal type\n    Given deal type is Merger of equal\n    And Buyer file is missing\n    When the system validates consolidation\n    Then display error \"Buyer data is mandatory for Merger of equal\"",
    "Acceptance_Criteria": "1. File format must be .xls or .xlsx.\n2. Mandatory columns:Employee ID, Job Title, Job Description, Function, Sub-Function, Cost Center, Address, Job Office City, Fully Loaded Cost, Currency, Employee Type.\n3. Status indicators: Missing, Uploaded, Needs Review.\n4. Chunked upload for large files with checksum validation.\n5. Uploading Buyer file not mandatory for Tuck in deal type\n6. If the deal type is merger of equals then uploading buyer file is mandatory",
    "Sprint": 2.0,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-026",
    "Epic": "HC Analysis",
    "Tool_Process": "Census",
    "Feature": "File Upload for analysis: Upload Target Census file",
    "User_Story": "As a user, I want to upload the Target census file \nSo that I can provide Target data for HC analysis.",
    "Description": null,
    "Scenario": "Scenario 1: Successfully upload Target Census file\n    Given HC analysis is selected\n    When the user uploads Target Census file in .xlsx format\n    Then the system should accept the file\n    And display status as \"Uploaded\"\n\n Scenario 2: File missing mandatory columns\n    Given the user uploads Buyer Census file\n    When mandatory columns are missing\n    Then the system should reject the file\n    And display error message \"Missing required columns: Employee ID, Job Title, etc.\"\n\n  Scenario 3: Large file upload with chunking\n    Given the Buyer Census file has more than 100k rows\n    When the system processes the upload\n    Then the file should be split into chunks\n    And each chunk verified using checksum\n\n  Scenario 4: Target file missing\n    Given HC analysis is selected\n    And Target Census file is not uploaded\n    When the user clicks \"Save & Continue\"\n    Then the system should display error \"Target Census file is mandatory\"\n\n Scenario 5: Missing entity data for deal type\n    Given deal type is Tuck-in/Acquisition\n    And Target file is missing\n    When the system validates consolidation\n    Then display error \"Target data is mandatory for Tuck-in/Acquisition\"",
    "Acceptance_Criteria": "1. File format must be .xls or .xlsx.\n2. Mandatory columns:Employee ID, Job Title, Job Description, Function, Sub-Function, Cost Center, Address, Job Office City, Fully Loaded Cost, Currency, Employee Type.\n3. Status indicators: Missing, Uploaded, Needs Review.\n4. Chunked upload for large files with checksum validation.\n5. Mandatory for all deal types.\n6. Status indicators and chunked upload logic apply.",
    "Sprint": 2.0,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-027",
    "Epic": "HC Analysis",
    "Tool_Process": "Census",
    "Feature": "File Upload for analysis: Validate Mandatory fields",
    "User_Story": "As a system, I want to check for missing mandatory fields in the  dataset So that I can ensure data completeness before ingestion.",
    "Description": null,
    "Scenario": " Scenario 1: All mandatory fields present\n    Given the merged dataset is created\n    When the system validates the file\n    Then status should be \"Uploaded\"\n    And no errors displayed\n\n  Scenario 2: Missing mandatory fields\n    Given the merged dataset has rows with blank Employee ID\n    When the system validates the file\n    Then status should be \"Needs Review\"\n    And display message \"Some rows have missing data. Update file or override to proceed\"",
    "Acceptance_Criteria": "1. Missing mandatory fields → Status = Needs Review.\n2. Display message: “Some rows have missing data. Update file or override to proceed.”\n3. Override option available with confirmation and audit logging.",
    "Sprint": 2.0,
    "Effort_Point": 5.0,
    "Effort_Required": 40.0,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-028",
    "Epic": "HC Analysis",
    "Tool_Process": "Census",
    "Feature": "File Upload for analysis: Consolidated Buyer and target files",
    "User_Story": "As a system, I want to merge Buyer and Target census files into a single dataset So that I can prepare data for ingestion and analysis.",
    "Description": null,
    "Scenario": " Scenario 1: Merge Buyer and Target files successfully\n    Given Buyer and Target Census files are uploaded\n     And deal type is Merger of Equals\n    When the system consolidates the files\n    Then the merged dataset should include an \"Entity Type\" column\n    And rows tagged as Buyer or Target correctly\n\n Scenario 2: Merge Buyer and Target files successfully\n    Given Buyer and Target Census files are uploaded\n     When deal type is Tuck-in/Acquisition\n     Then the System should not consolidate the buyer and target files",
    "Acceptance_Criteria": "1. Buyer and Target consolidated to one file if the deal type is Merger of Equals\n2. Add Entity Type column (Buyer/Target).\nValidate presence of required entities:\n3. Merger of Equals → Both Buyer and Target required.\nTuck-in/Acquisition → Target required; Buyer optional.\n4. Error message if required entity missing:\n“Target data is mandatory for Tuck-in/Acquisition.”\n5. If the deal type is Tuck-in the buyer and target data should not be consolidated.",
    "Sprint": 2.0,
    "Effort_Point": 5.0,
    "Effort_Required": 40.0,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": "Completed",
    "Comments": null
  },
  {
    "ID": "US-029",
    "Epic": "HC Analysis",
    "Tool_Process": "Census",
    "Feature": "File Upload for analysis: Handle large file uploads",
    "User_Story": "As a system, I want to support chunked uploads and integrity checks for large files\nSo that I can ensure reliable upload of up to 300k rows.",
    "Description": null,
    "Scenario": " Scenario 1: Upload large file successfully\n    Given the Buyer Census file has 300k rows\n    When the system processes the upload\n    Then the file should be split into chunks\n    And each chunk verified using checksum\n    And upload completes successfully\n\n  Scenario 2: Checksum mismatch detected\n    Given the system processes chunked upload\n    When checksum validation fails for a chunk\n    Then display error \"Upload failed due to integrity check. Please retry\"",
    "Acceptance_Criteria": "1. Chunk size configurable (300K rows).\n2. Checksum validation for each chunk.\n3. Upload fails if checksum mismatch detected.\n4. Display error:\n“Upload failed due to integrity check. Please retry.”",
    "Sprint": 2.0,
    "Effort_Point": 3.0,
    "Effort_Required": 24.0,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-030",
    "Epic": "HC Analysis",
    "Tool_Process": "Census",
    "Feature": "File Upload for analysis: Display control totals",
    "User_Story": "As a user, I want to see control totals after validation\nSo that I can verify data consistency before proceeding.",
    "Description": null,
    "Scenario": " Scenario: Show totals after successful validation\n    Given the merged dataset is validated\n    When the system completes processing\n    Then display total number of employees and total fully loaded cost\n    And group totals by Buyer and Target",
    "Acceptance_Criteria": "1. Display:\n      Total number of employees.\n      Total fully loaded cost.\n2. Grouped by Buyer and Target.\n3. Highlight discrepancies if totals differ significantly from expected values.",
    "Sprint": 2.0,
    "Effort_Point": 1.0,
    "Effort_Required": 8.0,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-031",
    "Epic": "Align Census fields to EY L1 Taxonomy",
    "Tool_Process": "Census (old)",
    "Feature": "Review census file fields for EY L1 taxonomy mapping",
    "User_Story": "As a user, I want to review all fields in the client-provided census file so that I can identify the field that most closely corresponds to EY L1 taxonomy.",
    "Description": null,
    "Scenario": "Scenario: Successfully load and review census file fields\n    Given a client-provided census file is available in the system\n    When I open the file for review\n    Then I should see all available fields (e.g., Function, Sub function, Cost Center, etc.)\n    And I should be able to compare them against EY L1 taxonomy definitions",
    "Acceptance_Criteria": "1. The system must allow uploading or accessing the client census file.\n2. All fields in the file must be displayed clearly for review.\n3. EY L1 taxonomy definitions must be accessible for comparison.\n4. The analyst can mark or select the field that corresponds most closely to EY L1 taxonomy.\n5. If no matching field exists, the analyst can flag it as “No Match”.\n6. If the file is missing expected fields (e.g., Function), the system should display a warning.\n7.If the file contains duplicate field names, the system should allow differentiation.",
    "Sprint": 2.0,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-032",
    "Epic": "Align Census fields to EY L1 Taxonomy",
    "Tool_Process": "Census (old)",
    "Feature": "Validate field mapping to EY L1 taxonomy",
    "User_Story": "As a User, I want to validate the selected field mapping so that the mapping is accurate and consistent with EY L1 taxonomy.",
    "Description": null,
    "Scenario": "Scenario: Validate selected field mapping\n    Given I have selected a field that corresponds to EY L1 taxonomy\n    When I confirm the mapping\n    Then the system should validate that the selected field exists in the census file\n    And the mapping should be stored for reporting",
    "Acceptance_Criteria": "1. The system must confirm the selected field exists in the census file.\n2. The mapping must be saved in a structured format (e.g., JSON or database).\n3. If multiple fields partially match EY L1 taxonomy, the system should allow ranking or comments.\n4. If the selected field is empty or contains invalid data, the system should prompt for",
    "Sprint": 2.0,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-033",
    "Epic": "Store Client Function Taxonomy (EY L1 Equivalent)",
    "Tool_Process": "Census (old)",
    "Feature": "Store client function taxonomy",
    "User_Story": "As a user, I want to store the client function taxonomy (EY L1 equivalent) so that it can be referenced consistently across processes.",
    "Description": null,
    "Scenario": " Scenario: Successfully store client function taxonomy\n    Given I have identified the client function equivalent to EY L1 taxonomy\n    When I save the client function taxonomy in the system\n    Then the system should store the taxonomy set as \"client function\"\n    And it should be retrievable for future mapping and reporting",
    "Acceptance_Criteria": "1. The system must allow saving a set of client functions mapped to EY L1 taxonomy.\n2. The stored taxonomy must be labeled as “client function”.\n3. The taxonomy must be retrievable for use in other processes (e.g., mapping, reporting).\n4. If the taxonomy set is empty, the system should prevent saving and display an error.\n5. If duplicate entries exist in the taxonomy, the system should allow saving but flag duplicates for review.\n6. If the taxonomy contains invalid characters or exceeds length limits, the system should validate and prompt correction.",
    "Sprint": 2.0,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-034",
    "Epic": "Store Client Function Taxonomy (EY L1 Equivalent)",
    "Tool_Process": "Census (old)",
    "Feature": "Update client function taxonomy",
    "User_Story": "As a user, I want to update the stored client function taxonomy so that changes in client data or EY taxonomy can be reflected.",
    "Description": null,
    "Scenario": "Scenario: Successfully update client function taxonomy\n    Given a client function taxonomy set is already stored\n    When I modify the taxonomy entries\n    Then the system should update the stored taxonomy\n    And maintain version history for audit purposes",
    "Acceptance_Criteria": "1. The system must allow editing existing taxonomy entries.\n2. Updates must be saved and overwrite previous values.\n3. Version history must be maintained for audit and rollback.\n4.  If an update removes all entries, the system should prompt confirmation before saving.\n5. If multiple users attempt to update simultaneously, the system should handle concurrency (e.g., lock or merge changes).",
    "Sprint": 2.0,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-035",
    "Epic": "Identify Employee unique identifier",
    "Tool_Process": "Census (old)",
    "Feature": "Identify Existing Unique employee Identifier",
    "User_Story": "As a user, I want to review fields in the client census file to identify an existing unique identifier so that each employee can be referenced consistently throughout the process.\n",
    "Description": null,
    "Scenario": "Scenario 1: Field exists and is valid\n  Given the census file contains a field named \"Employee ID\"\n  And all values in this field are unique and non-null\n  When I select this field as the unique identifier\n  Then the system should accept the selection\n  And confirm the identifier is valid\n\nScenario 2: Field exists but contains duplicates\n  Given the census file contains a field named \"Employee ID\"\n  And some values in this field are duplicated\n  When I attempt to select this field as the unique identifier\n  Then the system should reject the selection\n  And display an error message indicating duplicates\n\nScenario 3: Field exists but contains null values\n  Given the census file contains a field named \"Employee ID\"\n  And some values in this field are null or empty\n  When I attempt to select this field as the unique identifier\n  Then the system should reject the selection\n  And prompt me to choose another field or generate an identifier\n\nScenario 4: Multiple candidate fields exist\n  Given the census file contains fields \"Employee ID\" and \"WWID\"\n  And both fields have unique values\n  When I review the fields\n  Then the system should allow me to select one as the unique identifier",
    "Acceptance_Criteria": "1. The selected field must exist in the census file.\n2. All values in the selected field must be unique.\n3. No null or empty values in the selected field.\n4. System confirms selection and marks the field as the unique identifier.\n5. System must validate uniqueness before accepting the field.\n6. If duplicates exist, system rejects the selection.\n7. System displays an error message: “Selected field contains duplicate values. Please choose another field or generate a unique identifier.”\n8. System must validate for null or empty values.\n9. If nulls exist, system rejects the selection.\nSystem displays an error message: “Selected field contains missing values. Please choose another field or generate a unique identifier.”\n10. System must allow user to view all candidate fields.\n11. User can select one field as the unique identifier.\n12. System validates the selected field before confirming.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-036",
    "Epic": "Identify Employee unique identifier",
    "Tool_Process": "Census (old)",
    "Feature": "Generate Unique Identifier When None Exists",
    "User_Story": "As a user, I want to create a unique identifier if none exists so that each employee can be referenced consistently throughout the process.",
    "Description": null,
    "Scenario": "Scenario 1: No suitable field exists\n  Given the census file contains fields \"Name\" and \"Department\"\n  And none of these fields have unique values\n  When I request the system to generate a unique identifier\n  Then the system should create a new column with unique IDs for each employee\n  And ensure IDs remain constant throughout the process\n\nScenario  2: File is very large\n  Given the census file contains 1 million rows\n  When I request the system to generate unique identifiers\n  Then the system should generate IDs efficiently without performance degradation\n\nScenario 3: Similar generated ID already exists\n  Given the census file contains a column named \"Generated_ID\"\n  When I request the system to create unique identifiers\n  Then the system should avoid duplication and create a distinct identifier column",
    "Acceptance_Criteria": "1. System must allow user to trigger ID generation.\n2. System generates a unique identifier for each row (e.g., UUID or sequential ID).\n3. Generated IDs must be unique and non-null.\n4. Generated IDs must remain constant throughout the process.\n5. System adds the new identifier column to the census file.\n6. System must generate IDs without performance degradation (e.g., within acceptable time limits).\n7. System must not crash or timeout during generation.\n8. System checks for existing columns with similar names (e.g., “Generated_ID”).\n9. If found, system creates a distinct column name (e.g., “Generated_ID_1”).\n10. System ensures no duplication of values.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-037",
    "Epic": "Identify Employee unique identifier",
    "Tool_Process": "Census (old)",
    "Feature": "Validate Identifier Integrity",
    "User_Story": "As a user, I want to validate the uniqueness and consistency of the identifier so that it can be reliably used for mapping and reporting.",
    "Description": null,
    "Scenario": "Scenario 1: Validation passes\n  Given the census file contains a unique identifier column\n  And all values are unique and non-null\n  When I run validation\n  Then the system should confirm the identifier integrity\n  And display a success message\n\nScenario 2: Validation fails due to duplicates\n  Given the census file contains a unique identifier column\n  And some values are duplicated\n  When I run validation\n  Then the system should display an error message\n  And prevent further processing until corrected\n\nScenario 3: Validation fails due to null values\n  Given the census file contains a unique identifier column\n  And some values are null\n  When I run validation\n  Then the system should display an error message",
    "Acceptance_Criteria": "1. All identifiers must be unique.\n2. No null or empty values.\n3. Identifier format must be consistent (e.g., all UUIDs or all numeric).\n4. System displays success message: “Validation passed. Identifier integrity confirmed.”\n5. System detects duplicate values.\n6. System displays error message: “Validation failed: Duplicate identifiers found.”\n7. System prevents further processing until corrected.\n8. System detects null or empty values.\n9. System displays error message: “Validation failed: Missing identifiers found.”\n10. System prevents further processing until corrected.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-038",
    "Epic": "Map Client Functions to EY Standardized Taxonomy (EY L1)",
    "Tool_Process": "Census (old)",
    "Feature": "Identify functional assignment from client census",
    "User_Story": "As a user, I want to identify individuals’ functional assignment in the client census file so that I can determine their client function for mapping to EY L1 taxonomy.",
    "Description": null,
    "Scenario": "Scenario: Successfully extract client function fields\n    Given a client census file is available\n    When I review fields such as Function, Sub function, and Cost Center\n    Then I should identify the field(s) representing client function for each individual",
    "Acceptance_Criteria": "1. System must display all relevant fields (Function, Sub function, Cost Center).\n2. Analyst can select one or more fields that represent client function.\n3. If multiple fields exist, system should allow prioritization or combination logic.\n4.  If no relevant fields exist, system should prompt to upload a valid file or manually input function data.\n5.  If fields contain null or inconsistent values, system should flag and allow correction.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-039",
    "Epic": "Map Client Functions to EY Standardized Taxonomy (EY L1)",
    "Tool_Process": "Census (old)",
    "Feature": "Evaluate client taxonomy against EY L1",
    "User_Story": "As a user, I want to evaluate the client-provided functional taxonomy or LLM client function so that I can identify the best-fit EY L1 taxonomy level.\n",
    "Description": null,
    "Scenario": "Scenario: Successfully identify best-fit EY L1 taxonomy\n    Given client function taxonomy is available\n    And EY standardized taxonomy is accessible\n    When I compare client functions to EY L1 taxonomy\n    Then I should identify the best-fit EY L1 level for each client function",
    "Acceptance_Criteria": "1. EY L1 taxonomy definitions must be accessible in the system.\n2. System must allow mapping of each client function to an EY L1 equivalent.\n3. If multiple EY L1 options exist, system should allow ranking or comments.\n4. If no suitable EY L1 match exists, system should allow marking as “No Match” and flag for review.\n5.  If client taxonomy contains duplicates or invalid entries, system should prompt correction before mapping.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-040",
    "Epic": "Map Client Functions to EY Standardized Taxonomy (EY L1)",
    "Tool_Process": "Census (old)",
    "Feature": "Map individuals to EY L1 taxonomy",
    "User_Story": "As a user, I want to map individuals in the client census from client function to EY standardized function (EY L1) so that reporting aligns with EY taxonomy.",
    "Description": null,
    "Scenario": "Scenario 1: Individual has multiple client functions\n  Given an individual is assigned to \"Finance\" and \"HR\"\n  When I apply mapping\n  Then the system should allow multiple EY L1 assignments\n  Or prompt for primary function selection\n\nScenario 2: Individual has no client function assigned\n  Given an individual record has a blank function field\n  When I apply mapping\n  Then the system should flag the record\n  And allow manual assignment before proceeding",
    "Acceptance_Criteria": "1. System must apply mapping consistently across all individuals.\n 2. Output must include:\n         Original client function\n          Mapped EY L1 function\n3. Mapping must remain constant throughout the process.\n4. If an individual’s client function is unmapped, system should flag and allow manual assignment.\n5.  If multiple mappings exist for the same client function, system should prompt for resolution.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-041",
    "Epic": "Normalize employee type",
    "Tool_Process": "Census (old)",
    "Feature": "Evaluate client employee type taxonomy",
    "User_Story": "As a user I want to evaluate the client-provided employee type taxonomy So that I can identify the best matching EY standardized employee type for accurate mapping",
    "Description": null,
    "Scenario": "Scenario 1: Map client taxonomy to EY standard taxonomy\n  Given client provides an employee type taxonomy with valid values\n  When the system compares client taxonomy with EY standardized taxonomy\n  Then the system should identify the closest matching EY employee type for each client type\n  And the mapping should be saved for reporting\n\nScenario 2: Flag unmapped employee types\n  Given client taxonomy contains values not present in EY taxonomy\n  When the system attempts to map these values\n  Then the system should flag them as unmapped\n  And log them for manual review\n\nScenario 3: Multiple EY matches for a client type\n  Given a client employee type matches more than one EY type equally\n  When the system applies mapping rules\n  Then the system should flag the ambiguity\n  And request manual resolution\n\nScenario 4: Null or blank employee type values\n  Given client taxonomy contains null or blank employee type values\n  When the system processes the mapping\n  Then the system should exclude these records from mapping\n  And log them for data quality review",
    "Acceptance_Criteria": "1. System must validate client taxonomy format (CSV, Excel, JSON).\n2. Exact matches should be prioritized; best-fit logic applied if no exact match.\n3. Unmapped or ambiguous values must be flagged for manual review.\n4. Null or blank values must be excluded and logged.\n5. Mapping must be auditable and version-controlled.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-042",
    "Epic": "Normalize employee type",
    "Tool_Process": "Census (old)",
    "Feature": "Identify Employee Type Alignment",
    "User_Story": "As a user, I want to identify individuals’ employee type alignment using the correct field\nSo that I can distinguish between full-time employees and contractors accurately",
    "Description": null,
    "Scenario": "Scenario 1: Use Employee type field for alignment\n  Given client census data contains 'Employee type' field\n  When the system checks for employment status\n  Then the system should classify individuals as full-time or contractor based on this field\n\nScenario 2: Use Worker type field for alignment\n  Given client census data does not contain 'Employee type' field\n  And contains 'Worker type' field\n  When the system checks for employment status\n  Then the system should classify individuals based on 'Worker type'\n\nScenario 3: Missing both Employee type and Worker type fields\n  Given client census data does not contain 'Employee type' or 'Worker type'\n  When the system attempts to classify individuals\n  Then the system should return an error\n  And log the issue for manual intervention\n\nScenario 4: Normalize inconsistent field values\n  Given 'Employee type' field contains values like \"FT\", \"Fulltime\", \"Contractor\"\n  When the system processes these values\n  Then the system should normalize them using a predefined mapping dictionary\n  And classify individuals correctly",
    "Acceptance_Criteria": "1. System must check for 'Employee type' or 'Worker type' fields.\n2. If both exist, prioritize 'Employee type' over 'Worker type'.\n3. Missing both fields should trigger an error and log.\n4. Normalize inconsistent values using a mapping dictionary.\n5. Handle case sensitivity and special characters.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-043",
    "Epic": "Normalize employee type",
    "Tool_Process": "Census (old)",
    "Feature": "Map Individuals to EY Standard Employee Type",
    "User_Story": "As a User, I want to map individuals from client census to EY standardized employee type\nSo that reporting and analytics are consistent across systems",
    "Description": null,
    "Scenario": "Scenario 1: Map individuals to EY standard employee type\n  Given client census data contains valid employee type values\n  When the system applies mapping rules\n  Then each individual should be assigned an EY employee type (Employee or Non-Employee)\n\nScenario 2: Missing employee type for an individual\n  Given client census data contains records with missing employee type\n  When the system processes these records\n  Then the system should assign 'Unknown' as EY employee type\n  And flag for review\n\nScenario 3: Ambiguous mapping for an individual\n  Given an individual's employee type maps to multiple EY types\n  When the system applies mapping rules\n  Then the system should log ambiguity\n  And request manual resolution\n\nScenario 4: Generate mapping report\n  Given mapping is completed for all individuals\n  When the system generates the report\n  Then the report should include original value, mapped value, and confidence level\n  And be auditable with version control",
    "Acceptance_Criteria": "1. Mapping rules must be applied consistently for all records.\n2. Missing employee type should default to 'Unknown' and be flagged.\n3. Ambiguous mappings must be logged for manual resolution.\n4. Generate a detailed mapping report with original value, mapped value, confidence level.\n5. Ensure auditability and version control for mapping changes.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-044",
    "Epic": "Normalize GBS v. Corporate (if applicable)",
    "Tool_Process": "Census (old)",
    "Feature": "Evaluate Client Worker Type Taxonomy",
    "User_Story": "As a user, I want to evaluate the client-provided worker type taxonomy, including fields related to GBS or shared services\nSo that I can identify the best matching EY standardized worker type for accurate mapping",
    "Description": null,
    "Scenario": "Scenario 1: Map client worker taxonomy to EY standard taxonomy\n  Given client provides a worker type taxonomy with valid values\n  When the system compares client taxonomy with EY standardized taxonomy\n  Then the system should identify the closest matching EY worker type for each client type\n  And the mapping should be saved for reporting\n\nScenario 2: Flag unmapped worker types\n  Given client taxonomy contains values not present in EY taxonomy\n  When the system attempts to map these values\n  Then the system should flag them as unmapped\n  And log them for manual review\n\nScenario 3: Multiple EY matches for a client worker type\n  Given a client worker type matches more than one EY type equally\n  When the system applies mapping rules\n  Then the system should flag the ambiguity\n  And request manual resolution\n\nScenario 4: Null or blank worker type values\n  Given client taxonomy contains null or blank worker type values\n  When the system processes the mapping\n  Then the system should exclude these records from mapping\n  And log them for data quality review",
    "Acceptance_Criteria": "1. System must validate client worker taxonomy format (CSV, Excel, JSON).\n2. Exact matches should be prioritized; best-fit logic applied if no exact match.\n3. Unmapped or ambiguous values must be flagged for manual review.\n4. Null or blank values must be excluded and logged.\n5. Mapping must be auditable and version-controlled.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-045",
    "Epic": "Normalize GBS v. Corporate (if applicable)",
    "Tool_Process": "Census (old)",
    "Feature": "Identify Worker Type Alignment",
    "User_Story": "As a user. I want to identify individuals’ worker type alignment using the correct field\nSo that I can distinguish between GBS/Shared Service and Corporate/HQ accurately",
    "Description": null,
    "Scenario": "Scenario 1: Use Employee type or Worker type field for alignment\n  Given client census data contains 'Employee type' or 'Worker type' field\n  When the system checks for organizational alignment\n  Then the system should classify individuals as GBS/Shared Service or Corporate/HQ based on this field\n\nScenario 2: Missing both Employee type and Worker type fields\n  Given client census data does not contain 'Employee type' or 'Worker type'\n  When the system attempts to classify individuals\n  Then the system should return an error\n  And log the issue for manual intervention\n\nScenario 3: Normalize inconsistent field values\n  Given 'Worker type' field contains values like \"GBS\", \"SharedSvc\", \"Corp\", \"HQ\"\n  When the system processes these values\n  Then the system should normalize them using a predefined mapping dictionary\n  And classify individuals correctly",
    "Acceptance_Criteria": "1. System must check for 'Employee type' or 'Worker type' fields.\n2. If both exist, prioritize 'Worker type' for GBS/Corporate classification.\n3. Missing both fields should trigger an error and log.\n4. Normalize inconsistent values using a mapping dictionary.\n5. Handle case sensitivity and special characters.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-046",
    "Epic": "Normalize GBS v. Corporate (if applicable)",
    "Tool_Process": "Census (old)",
    "Feature": "Map Individuals to EY Standard Worker Type",
    "User_Story": "As a user, I want to map individuals from client census to EY standardized worker type So that reporting and analytics are consistent across systems",
    "Description": null,
    "Scenario": "Scenario 1: Map individuals to EY standard worker type\n  Given client census data contains valid worker type values\n  When the system applies mapping rules\n  Then each individual should be assigned an EY worker type (GBS, Corporate, or Unknown)\n\nScenario 2: Missing worker type for an individual\n  Given client census data contains records with missing worker type\n  When the system processes these records\n  Then the system should assign 'Unknown' as EY worker type\n  And flag for review\n\nScenario 3: Ambiguous mapping for an individual\n  Given an individual's worker type maps to multiple EY types\n  When the system applies mapping rules\n  Then the system should log ambiguity\n  And request manual resolution\n\nScenario 4: Generate mapping report\n  Given mapping is completed for all individuals\n  When the system generates the report\n  Then the report should include original value, mapped value, and confidence level\n  And be auditable with version control",
    "Acceptance_Criteria": "1. Mapping rules must be applied consistently for all records.\n2. Missing worker type should default to 'Unknown' and be flagged.\n3. Ambiguous mappings must be logged for manual resolution.\n4. Generate a detailed mapping report with original value, mapped value, confidence level.\n5. Ensure auditability and version control for mapping changes.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-047",
    "Epic": "Normalize currency if Toggle is on",
    "Tool_Process": "Census Update for Synergy",
    "Feature": "Normalize currency based on selection in project creation tab",
    "User_Story": "As a user, I want to normalize currency based on the selection in the project creation page\nSo that all cost data is standardized for reporting and analysis",
    "Description": null,
    "Scenario": "\nScenario 1: Normalize currency when standardization is toggled on\n  Given currency standardization is enabled in the project creation page\n  And USD is selected as the standardized currency\n  When the system processes cost data\n  Then all costs should be converted to USD using the latest conversion rate\n\nScenario 2: Skip normalization when standardization is toggled off\n  Given currency standardization is disabled in the project creation page\n  When the system processes cost data\n  Then the system should retain original currency values\n  And move to the next step without conversion",
    "Acceptance_Criteria": "1. Currency normalization should only occur if the toggle is ON.\n2. Selected standardized currency must be applied consistently across all records.\n3. If toggle is OFF, no conversion should occur.",
    "Sprint": 3.0,
    "Effort_Point": 10.0,
    "Effort_Required": 80.0,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": "Completed",
    "Comments": null
  },
  {
    "ID": "US-048",
    "Epic": "Normalize currency if Toggle is on",
    "Tool_Process": "Census Update for Synergy",
    "Feature": "Review Current Currency in Census File",
    "User_Story": "As a user, I want to review the currency field in the census file\nSo that I can determine the current currency before applying conversion",
    "Description": null,
    "Scenario": "Scenario: Identify current currency from census file\n  Given the census file contains a currency field\n  When the system reads the currency field\n  Then the system should determine the current currency for each record",
    "Acceptance_Criteria": "1. System must validate that the currency field exists in the census file.\n2. If the currency field is missing, log an error and halt normalization.\n3. Handle cases where currency values are inconsistent (e.g., \"USD\", \"US$\", \"Dollar\") by normalizing them.",
    "Sprint": 3.0,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": "Completed",
    "Comments": null
  },
  {
    "ID": "US-049",
    "Epic": "Normalize currency if Toggle is on",
    "Tool_Process": "Census Update for Synergy",
    "Feature": "Compare and Convert Currency",
    "User_Story": "As a user, I want to compare the current currency to the selected standardized currency\nSo that I can apply conversion only when needed",
    "Description": null,
    "Scenario": "Scenario 1: No conversion needed when currencies match\n  Given the current currency matches the selected standardized currency\n  When the system processes cost data\n  Then no conversion should be applied\n  And costs should remain unchanged\n\nScenario 2: Convert currency when currencies differ\n  Given the current currency differs from the selected standardized currency\n  When the system retrieves the latest conversion rate using Open AI web search API\n  Then the system should apply the conversion rate to fully loaded cost\n  And update the cost values to the standardized currency",
    "Acceptance_Criteria": "1. If current currency equals standardized currency, skip conversion.\n2. If different, retrieve conversion rate via open AI web search API provided by competetive Edge team\n3. Apply conversion accurately to fully loaded cost.\n4. Log conversion details (original currency, target currency, rate applied).\n5. Handle API failure gracefully (retry or flag for manual intervention).",
    "Sprint": 3.0,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": "Completed",
    "Comments": null
  },
  {
    "ID": "US-050",
    "Epic": "GLOBAL CLIENTS: Normalize location to defined high cost / low cost & region classification",
    "Tool_Process": "Census (old)",
    "Feature": "Country Location Classification and Cost of Living Tagging",
    "User_Story": "As a user, I want to identify the country location for each individual in the client census file\nSo that I can map them to cost-of-living classifications",
    "Description": null,
    "Scenario": "Scenario: Extract country location from census file\n  Given the client census file contains a country field\n  When the system reads the country field for each individual\n  Then the system should identify the country location accurately",
    "Acceptance_Criteria": "1. System must validate that the country field exists in the census file.\n2. If the country field is missing, log an error and skip classification.\n3. Normalize inconsistent country names (e.g., \"USA\", \"United States\", \"US\").",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-051",
    "Epic": "GLOBAL CLIENTS: Normalize location to defined high cost / low cost & region classification",
    "Tool_Process": "Census (old)",
    "Feature": "Country Location Classification and Cost of Living Tagging",
    "User_Story": "As a user, I want to map each country to its corresponding cost-of-living index value\nSo that I can classify countries as high-cost or low-cost",
    "Description": null,
    "Scenario": "Scenario: Map country to cost-of-living index\n  Given a reference table of cost-of-living index values for all countries\n  When the system matches the country from the census file to the reference table\n  Then the system should retrieve the correct cost-of-living index value",
    "Acceptance_Criteria": "1. System must use a validated reference table for cost-of-living index values.\n2. If a country is not found in the reference table, log as \"Unknown\" and flag for review.\n3. Handle case sensitivity and alternate country names.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-052",
    "Epic": "GLOBAL CLIENTS: Normalize location to defined high cost / low cost & region classification",
    "Tool_Process": "Census (old)",
    "Feature": "Country Location Classification and Cost of Living Tagging",
    "User_Story": "As a user, I want to calculate the global 3rd quartile (3Q) value of cost-of-living index\nSo that I can use it as a threshold for high-cost vs low-cost classification",
    "Description": null,
    "Scenario": "Scenario: Calculate global 3rd quartile value\n  Given a list of cost-of-living index values for all countries globally\n  When the system calculates the 3rd quartile value\n  Then the system should store this value for classification",
    "Acceptance_Criteria": "1. System must calculate the 3rd quartile (75th percentile) accurately.\n2. Calculation should exclude null or invalid values.\n3. Log the calculated 3Q value for audit purposes.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-053",
    "Epic": "GLOBAL CLIENTS: Normalize location to defined high cost / low cost & region classification",
    "Tool_Process": "Census (old)",
    "Feature": "Country Location Classification and Cost of Living Tagging",
    "User_Story": "As a user,  I want to classify each country as high-cost or low-cost based on the 3Q threshold\nSo that I can tag individuals accordingly",
    "Description": null,
    "Scenario": "Scenario 1: Mark country as high-cost\n  Given the cost-of-living index for a country is greater than the global 3Q value\n  When the system processes the classification\n  Then the country should be marked as \"High Cost\"\n\nScenario 2: Mark country as low-cost\n  Given the cost-of-living index for a country is less than or equal to the global 3Q value\n  When the system processes the classification\n  Then the country should be marked as \"Low Cost\"",
    "Acceptance_Criteria": "1. Countries with index > 3Q → High Cost.\n2. Countries with index ≤ 3Q → Low Cost.\n3. Classification must be logged for audit.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-054",
    "Epic": "GLOBAL CLIENTS: Normalize location to defined high cost / low cost & region classification",
    "Tool_Process": "Census (old)",
    "Feature": "Country Location Classification and Cost of Living Tagging",
    "User_Story": "As a user, I want to tag individuals in the census file as high-cost or low-cost based on their country\nSo that cost analysis can be normalized across regions",
    "Description": null,
    "Scenario": "Scenario: Tag individuals based on country classification\n  Given each individual has a country classification of High Cost or Low Cost\n  When the system processes the census file\n  Then the individual should be tagged accordingly",
    "Acceptance_Criteria": "1. Each individual must inherit the classification of their country.\n2. If country classification is \"Unknown\", tag individual as \"Unknown\".\n3. Generate a report showing individual ID, country, classification.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-055",
    "Epic": " US ONLY CLIENTS: Normalize location to defined high cost / low cost & region classification",
    "Tool_Process": "Census (old)",
    "Feature": "State Location Classification and Cost of Living Tagging (USA Only)",
    "User_Story": "As a user, I want to identify the state location for each individual in the client census file\nSo that I can classify them based on cost-of-living index for U.S. states",
    "Description": null,
    "Scenario": "Scenario 1: Extract state location when country is USA\n  Given the client census file contains a country field and a state field\n  And the country is \"USA\"\n  When the system reads the state field for each individual\n  Then the system should identify the state location accurately\n\nScenario 2: Ignore state classification when country is not USA\n  Given the client census file contains a country field\n  And the country is not \"USA\"\n  When the system processes location data\n  Then the system should skip state classification",
    "Acceptance_Criteria": "1. State classification should only occur if country = USA.\n2. If state field is missing for USA records, log error and tag as \"Unknown\".\n3. Normalize inconsistent state names (e.g., \"CA\", \"California\").",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-056",
    "Epic": " US ONLY CLIENTS: Normalize location to defined high cost / low cost & region classification",
    "Tool_Process": "Census (old)",
    "Feature": "State Location Classification and Cost of Living Tagging (USA Only)",
    "User_Story": "As a user, I want to map each U.S. state to its corresponding cost-of-living index value\nSo that I can classify states as high-cost or low-cost",
    "Description": null,
    "Scenario": "Scenario: Map state to cost-of-living index\n  Given a reference table of cost-of-living index values for all U.S. states\n  When the system matches the state from the census file to the reference table\n  Then the system should retrieve the correct cost-of-living index value",
    "Acceptance_Criteria": "1. System must use a validated reference table for U.S. states.\n2. If a state is not found in the reference table, log as \"Unknown\" and flag for review.\n3. Handle case sensitivity and alternate state names.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-057",
    "Epic": " US ONLY CLIENTS: Normalize location to defined high cost / low cost & region classification",
    "Tool_Process": "Census (old)",
    "Feature": "State Location Classification and Cost of Living Tagging (USA Only)",
    "User_Story": "As a user, I want to calculate the 3rd quartile (3Q) value of cost-of-living index for U.S. states\nSo that I can use it as a threshold for high-cost vs low-cost classification",
    "Description": null,
    "Scenario": "Scenario: Calculate 3rd quartile value for U.S. states\n  Given a list of cost-of-living index values for all U.S. states\n  When the system calculates the 3rd quartile value\n  Then the system should store this value for classification",
    "Acceptance_Criteria": "1. System must calculate the 3rd quartile (75th percentile) accurately.\n2. Calculation should exclude null or invalid values.\n3. Log the calculated 3Q value for audit purposes.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-058",
    "Epic": " US ONLY CLIENTS: Normalize location to defined high cost / low cost & region classification",
    "Tool_Process": "Census (old)",
    "Feature": "State Location Classification and Cost of Living Tagging (USA Only)",
    "User_Story": "As a User, I want to classify each U.S. state as high-cost or low-cost based on the 3Q threshold\nSo that I can tag individuals accordingly",
    "Description": null,
    "Scenario": "Scenario 1: Mark state as high-cost\n  Given the cost-of-living index for a state is greater than the 3Q value\n  When the system processes the classification\n  Then the state should be marked as \"High Cost\"\n\nScenario 2: Mark state as low-cost\n  Given the cost-of-living index for a state is less than or equal to the 3Q value\n  When the system processes the classification\n  Then the state should be marked as \"Low Cost\"",
    "Acceptance_Criteria": "1. States with index > 3Q → High Cost.\n2. States with index ≤ 3Q → Low Cost.\n3. Classification must be logged for audit.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-059",
    "Epic": " US ONLY CLIENTS: Normalize location to defined high cost / low cost & region classification",
    "Tool_Process": "Census (old)",
    "Feature": "State Location Classification and Cost of Living Tagging (USA Only)",
    "User_Story": "As a user, I want to tag individuals in the census file as high-cost or low-cost based on their state\nSo that cost analysis can be normalized across regions",
    "Description": null,
    "Scenario": "Scenario: Tag individuals based on state classification\n  Given each individual has a state classification of High Cost or Low Cost\n  When the system processes the census file\n  Then the individual should be tagged accordingly",
    "Acceptance_Criteria": "1. Each individual must inherit the classification of their state.\n2. If state classification is \"Unknown\", tag individual as \"Unknown\".\n3. Generate a report showing individual ID, state, classification.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-060",
    "Epic": " US ONLY CLIENTS: Normalize location to defined high cost / low cost & region classification",
    "Tool_Process": "Census (old)",
    "Feature": "State Location Classification and Cost of Living Tagging (USA Only)",
    "User_Story": "As a user,  I want to map individuals to EYP normalized regions So that reporting aligns with EY regional standards",
    "Description": null,
    "Scenario": "Scenario: Map individuals to EYP normalized regions\n  Given a reference table of EYP normalized regions\n  When the system matches state or country to the corresponding region\n  Then the individual should be tagged with the correct EYP region",
    "Acceptance_Criteria": "1. System must use a validated EYP region mapping table.\n2. If mapping fails, tag as \"Unknown\" and log for review.\n3. Mapping must be auditable and version-controlled.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-061",
    "Epic": "Map Job title and Job description",
    "Tool_Process": "Census (old)",
    "Feature": "Job Title Mapping and Dynamic Job Description Creation",
    "User_Story": "As a user, I want to evaluate the client-provided job title taxonomy So that I can identify exact matches within EY standardized taxonomy or generic taxonomy and map individuals accordingly",
    "Description": null,
    "Scenario": "Scenario 1: Identify exact job title match in EY standardized taxonomy\n  Given the client census file contains job titles\n  And EY standardized taxonomy is available\n  When the system compares client job titles to EY taxonomy\n  Then the system should identify exact matches\n  And store the mapping for further processing\n\nScenario 2: Identify exact job title match in generic taxonomy for super-generic titles\n  Given the client census file contains job titles, roles, functions, and sub-functions\n  And the values are super-generic (e.g., \"IT Manager\", \"IT\")\n  When the system compares client job titles to generic taxonomy\n  Then the system should identify exact matches\n  And store the mapping for further processing",
    "Acceptance_Criteria": "1. System must validate presence of job title field in census file.\n2. Exact match should be prioritized in EY standardized taxonomy.\n3. If job title is generic, check against generic taxonomy.\n4. Log all matches for audit purposes.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-062",
    "Epic": "Map Job title and Job description",
    "Tool_Process": "Census (old)",
    "Feature": "Job Title Mapping and Dynamic Job Description Creation",
    "User_Story": "As a user, I want to map individuals to EY standardized job title or generic job title and corresponding job description\nSo that job roles are standardized for reporting and analytics",
    "Description": null,
    "Scenario": "Scenario: Map individuals when exact match is found\n  Given an exact job title match exists in EY standardized taxonomy or generic taxonomy\n  When the system processes the mapping\n  Then the individual should be assigned the corresponding EY job title\n  And the job description should be retrieved from the job description database",
    "Acceptance_Criteria": "1. If exact match exists in EY taxonomy or generic taxonomy, map job title and description.\n2. Job description must be retrieved from the job description database.\n3. Mapping must be logged and auditable.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-063",
    "Epic": "Map Job title and Job description",
    "Tool_Process": "Census (old)",
    "Feature": "Job Title Mapping and Dynamic Job Description Creation",
    "User_Story": "As a user, I want to create a dynamic job description based on reference points when no exact match is found\nSo that all roles have a standardized description",
    "Description": null,
    "Scenario": "Scenario: Create dynamic job description when no exact match exists\n  Given no exact job title match exists in EY standardized taxonomy or generic taxonomy\n  When the system processes the client job title, role, function, and sub-function\n  Then the system should generate a new job description dynamically\n  And assign it to the individual",
    "Acceptance_Criteria": "1. Dynamic job description must use reference points: client job title, role, function, sub-function.\n2. Generated description must meet EY formatting standards.\n3. Log creation of new job description for audit.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-064",
    "Epic": "Map Job title and Job description",
    "Tool_Process": "Census (old)",
    "Feature": "Job Title Mapping and Dynamic Job Description Creation",
    "User_Story": "As a user, I want to update the job description database with new job titles and descriptions\nSo that future mappings can reuse these entries",
    "Description": null,
    "Scenario": "Scenario: Update job description database with new entries\n  Given a new job description was created\n  When the system processes the update\n  Then the new job title and corresponding job description should be added to the database",
    "Acceptance_Criteria": "1. Database update must include new job title and description.\n2. Ensure version control and audit trail for updates.\n3. Validate that no duplicate entries are created.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-065",
    "Epic": "Map Individuals to Processes, Outcomes & Assumptions",
    "Tool_Process": "Census (old)",
    "Feature": "Map Individuals to Processes, Outcomes & Assumptions",
    "User_Story": "As a user, I want to use keyword mapping to match job descriptions to EY L2 and E2E L2 processes\nSo that individuals can be linked to standardized process taxonomy",
    "Description": null,
    "Scenario": "Scenario: Map job description to EY L2 and E2E L2 processes\n  Given the job description is available (from database or dynamically generated)\n  And EY standard taxonomy for L2 and E2E L2 processes is available\n  When the system performs keyword search on the job description\n  Then the system should identify the best matching EY L2 process\n  And identify the corresponding E2E L2 process",
    "Acceptance_Criteria": "1. System must validate presence of job description for each individual.\n2. Keyword mapping should use predefined dictionary or NLP-based matching.\n3. If no match found, log as \"Unmapped\" and flag for review.\n4. Mapping must be auditable.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-066",
    "Epic": "Map Individuals to Processes, Outcomes & Assumptions",
    "Tool_Process": "Census (old)",
    "Feature": "Map Individuals to Processes, Outcomes & Assumptions",
    "User_Story": "As a user I want to map EY L2 process to EY L1 process\nSo that I can classify job nature function",
    "Description": null,
    "Scenario": "Scenario: Map EY L2 process to EY L1 process\n  Given EY L2 process is identified for an individual\n  When the system references EY standard taxonomy\n  Then the system should map EY L2 process to EY L1 process\n  And store EY L1 as \"EY Job Nature Function\"",
    "Acceptance_Criteria": "1. EY L2 → EY L1 mapping must follow EY standard taxonomy.\n2. If mapping fails, log as \"Unknown\".\n3. Mapping must be stored for reporting.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-067",
    "Epic": "Map Individuals to Processes, Outcomes & Assumptions",
    "Tool_Process": "Census (old)",
    "Feature": "Map Individuals to Processes, Outcomes & Assumptions",
    "User_Story": "As a user, I want to map E2E L2 process to E2E L1 process\nSo that end-to-end process hierarchy is maintained",
    "Description": null,
    "Scenario": "Scenario: Map E2E L2 process to E2E L1 process\n  Given E2E L2 process is identified for an individual\n  When the system references EY standard taxonomy\n  Then the system should map E2E L2 process to E2E L1 process",
    "Acceptance_Criteria": "1. E2E L2 → E2E L1 mapping must follow EY standard taxonomy.\n2. If mapping fails, log as \"Unknown\".\n3. Mapping must be auditable.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-068",
    "Epic": "Map Individuals to Processes, Outcomes & Assumptions",
    "Tool_Process": "Census (old)",
    "Feature": "Map Individuals to Processes, Outcomes & Assumptions",
    "User_Story": "As a user, I want to map EY L2 process to work type, outcome L1, outcome L2, and assumptions\nSo that process-level details are standardized",
    "Description": null,
    "Scenario": "Scenario: Map EY L2 process to work type, outcomes, and assumptions\n  Given EY L2 process is identified for an individual\n  When the system references EY standard taxonomy\n  Then the system should map EY L2 process to:\n    | Work Type |\n    | Outcome L1 |\n    | Outcome L2 |\n    | Assumptions (GBS %, Automate %, Outsource %) |",
    "Acceptance_Criteria": "1. Mapping must include work type, outcome L1, outcome L2.\n2. Assumptions must include GBS low/high %, Automate low/high %, Outsource low/high %.\n3. All mappings must be stored and auditable.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-069",
    "Epic": "Map Individuals to Processes, Outcomes & Assumptions",
    "Tool_Process": "Census (old)",
    "Feature": "Map Individuals to Processes, Outcomes & Assumptions",
    "User_Story": "As a user, I want to store the same information for E2E L2 process So that both EY L2 and E2E L2 mappings are complete",
    "Description": null,
    "Scenario": "Scenario: Store outcomes and assumptions for E2E L2 process\n  Given EY L2 process mapping is completed\n  And E2E L2 process is identified\n  When the system processes mapping details\n  Then the system should store:\n    | Outcome L1 |\n    | Outcome L2 |\n    | Work Type |\n    | Assumptions (GBS %, Automate %, Outsource %) |\n  For the E2E L2 process as well",
    "Acceptance_Criteria": "1. E2E L2 process must inherit all mapped details from EY L2 process.\n2. Ensure consistency between EY L2 and E2E L2 mappings.\n3. Mapping must be auditable and version-controlled.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-070",
    "Epic": "Identify and tag shadow organization roles by function",
    "Tool_Process": "Census (old)",
    "Feature": "Identify Shadow Organization Roles and Centralization Opportunities",
    "User_Story": "As a user, I want to ensure each individual has both a normalized EY L1 (from step 2a) and a mapped EY L1 (job nature function from step 3b)\nSo that I can compare them for shadow organization identification",
    "Description": null,
    "Scenario": "Scenario: Check presence of normalized EY L1 and mapped EY L1\n  Given the client census file contains normalized EY L1 and mapped EY L1 for each individual\n  When the system validates these fields\n  Then the system should confirm both values exist\n  And log any missing values for review",
    "Acceptance_Criteria": "1. Both normalized EY L1 and mapped EY L1 must exist for every individual.\n2. If either is missing, log error and skip comparison for that individual.\n3. Generate a validation report listing missing fields.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-071",
    "Epic": "Identify and tag shadow organization roles by function",
    "Tool_Process": "Census (old)",
    "Feature": "Identify Shadow Organization Roles and Centralization Opportunities",
    "User_Story": "As a user I want to compare normalized EY L1 to mapped EY L1 So that I can identify mismatches indicating shadow organization roles",
    "Description": null,
    "Scenario": "Scenario: Identify mismatch between normalized EY L1 and mapped EY L1\n  Given normalized EY L1 and mapped EY L1 exist for an individual\n  When the system compares these values\n  Then if the values differ\n  The individual should be flagged as a shadow organization role",
    "Acceptance_Criteria": "1. If normalized EY L1 = mapped EY L1 → no action.\n2. If normalized EY L1 ≠ mapped EY L1 → flag as shadow org role.\n3. Log comparison results for audit.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-072",
    "Epic": "Identify and tag shadow organization roles by function",
    "Tool_Process": "Census (old)",
    "Feature": "Identify Shadow Organization Roles and Centralization Opportunities",
    "User_Story": "As a user, I want to mark flagged shadow org roles as opportunities for centralization\nSo that cost-saving opportunities can be identified",
    "Description": null,
    "Scenario": "Scenario: Mark shadow org roles for centralization\n  Given an individual is flagged as a shadow organization role\n  When the system processes the flag\n  Then the individual should be marked as an opportunity for centralization",
    "Acceptance_Criteria": "1. All flagged individuals must be categorized as centralization opportunities.\n2. Store this categorization for reporting and analytics.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-073",
    "Epic": "Identify and tag shadow organization roles by function",
    "Tool_Process": "Census (old)",
    "Feature": "Identify Shadow Organization Roles and Centralization Opportunities",
    "User_Story": "As a user, I want to categorize identified shadow org roles\nSo that potential cost-saving opportunities are clearly reported",
    "Description": null,
    "Scenario": "Scenario: Categorize shadow org roles for reporting\n  Given individuals are flagged as shadow org roles\n  When the system generates the report\n  Then the report should include:\n    | Individual ID |\n    | Normalized EY L1 |\n    | Mapped EY L1 |\n    | Shadow Org Flag |\n    | Centralization Opportunity |",
    "Acceptance_Criteria": "1. Report must include all flagged individuals with relevant details.\n2. Ensure auditability and version control for reports.\n3. Handle partial data gracefully (e.g., missing IDs or L1 values).",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-074",
    "Epic": "Configure Lever-Based Headcount Impact Settings",
    "Tool_Process": "Census (old)",
    "Feature": "Capture User Input for Lever Selection",
    "User_Story": "As a System User I want to select which levers (Outsource, Shift, Automate) will impact headcount\nSo that the system can configure process flows accordingly",
    "Description": null,
    "Scenario": "Scenario: Capture user input for lever selection\n  Given the project configuration page displays checkboxes for levers: Outsource, Shift, Automate\n  When the user selects or deselects each checkbox\n  Then the system should capture the selection for each lever",
    "Acceptance_Criteria": "1. UI must display checkboxes for all three levers.\n2. User selections must be captured accurately.\n3. Default state for each checkbox should be \"unchecked\".",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-075",
    "Epic": "Configure Lever-Based Headcount Impact Settings",
    "Tool_Process": "Census (old)",
    "Feature": "Capture User Input for Lever Selection",
    "User_Story": "As a System I want to set a \"lever being pulled\" value for each lever based on user input\nSo that these values can be referenced in downstream processes",
    "Description": null,
    "Scenario": "Scenario: Set lever being pulled value to Yes or No\n  Given the user has selected or deselected a checkbox for a lever\n  When the system processes the input\n  Then the system should set \"Lever being pulled - [Lever]\" to \"Yes\" if selected\n  And set it to \"No\" if not selected",
    "Acceptance_Criteria": "1. For each lever (Outsource, Shift, Automate), create a corresponding flag:\n          \"Lever being pulled - Outsource\"\n          \"Lever being pulled - Shift\"\n          \"Lever being pulled - Automate\"\n2. If checkbox is selected → set value to \"Yes\".\n3. If checkbox is not selected → set value to \"No\".\n4. Ensure values are stored for future reference.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-076",
    "Epic": "Configure Lever-Based Headcount Impact Settings",
    "Tool_Process": "Census (old)",
    "Feature": "Capture User Input for Lever Selection",
    "User_Story": "As a System, I want to store lever configuration values\nSo that they can be referenced in processes 5–9",
    "Description": null,
    "Scenario": "Scenario: Store lever configuration values\n  Given lever being pulled values are set for all three levers\n  When the system saves the configuration\n  Then the values should be stored in the database\n  And be retrievable for downstream processes",
    "Acceptance_Criteria": "1. Store lever configuration in a persistent data store.\n2. Ensure values are retrievable by processes 5–9.\n3. Maintain audit trail for changes in lever configuration.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-077",
    "Epic": "Calculate Shadow Organization Reassignment and Post-Reorganization Headcount",
    "Tool_Process": "Census (old)",
    "Feature": "Calculate Shadow Org Assignments and Post-Reassignment Headcount (Function-Level)",
    "User_Story": "As a user, I want to collate all L2 processes that map to at least one census row So that I can identify all processes performed by the client organization",
    "Description": null,
    "Scenario": "Scenario: Collate L2 processes mapped to census rows\n  Given the census file and process mapping from step 3a\n  When the system identifies all L2 processes linked to at least one census row\n  Then the system should return an array of L2 processes sorted by associated function",
    "Acceptance_Criteria": "1. System must return all L2 processes mapped to census rows.\n2. Sort L2 processes by associated function.\n3. If no L2 processes found, return an empty array and log.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-078",
    "Epic": "Calculate Shadow Organization Reassignment and Post-Reorganization Headcount",
    "Tool_Process": "Census (old)",
    "Feature": "Count Individuals Correctly Assigned to Function",
    "User_Story": "As a user, I want to count individuals whose normalized EY L1 matches EY job nature function So that I can identify correctly assigned headcount",
    "Description": null,
    "Scenario": "Scenario: Count individuals with matching normalized EY L1 and job nature function\n  Given census rows with normalized EY L1 and EY job nature function\n  When the system compares these values for each row\n  Then the system should count rows where both values match\n  And store the count as \"In function cost center with matching job nature\"",
    "Acceptance_Criteria": "1. Count rows where normalized EY L1 = EY job nature function.\n2. Store counts per L2 process and function.\n3. Log counts for audit and reporting.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-079",
    "Epic": "Calculate Shadow Organization Reassignment and Post-Reorganization Headcount",
    "Tool_Process": "Census (old)",
    "Feature": "Count Individuals with Shadow Org Roles (Outgoing)",
    "User_Story": "As a user, I want to count individuals where normalized EY L1 matches the function being summarized but EY job nature function does not So that I can identify shadow org roles that need to be centralized out",
    "Description": null,
    "Scenario": "Scenario: Count individuals in function cost center with other job nature\n  Given census rows with normalized EY L1 and EY job nature function\n  When the system processes rows for the function being summarized\n  Then count rows where normalized EY L1 matches the function AND EY job nature function differs\n  And store the count as \"In function cost center with other job nature\"",
    "Acceptance_Criteria": "1. Count rows where normalized EY L1 = function being summarized AND EY job nature ≠ function.\n2. Store counts per function.\n3. Log for reporting.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-080",
    "Epic": "Calculate Shadow Organization Reassignment and Post-Reorganization Headcount",
    "Tool_Process": "Census (old)",
    "Feature": "Count Individuals with Shadow Org Roles (Incoming)",
    "User_Story": "As a user, I want to count individuals where normalized EY L1 does NOT match the function being summarized but EY job nature function does So that I can identify shadow org roles that need to be centralized in",
    "Description": null,
    "Scenario": "Scenario: Count individuals with shadow headcount for incoming centralization\n  Given census rows with normalized EY L1 and EY job nature function\n  When the system processes rows for the function being summarized\n  Then count rows where normalized EY L1 ≠ function AND EY job nature function = function\n  And store the count as \"Total shadow headcount\"",
    "Acceptance_Criteria": "1. Count rows where normalized EY L1 ≠ function AND EY job nature = function.\n2. Store counts per L2 process and function.\n3. If count = 0, store as 0.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-081",
    "Epic": "Calculate Shadow Organization Reassignment and Post-Reorganization Headcount",
    "Tool_Process": "Census (old)",
    "Feature": "Aggregate Shadow Headcount by L2 Process",
    "User_Story": "As a user, I want to aggregate shadow headcount by L2 process and function So that I can provide detailed reporting",
    "Description": null,
    "Scenario": "Scenario: Aggregate shadow headcount by L2 process\n  Given rows counted as part of total shadow headcount\n  When the system groups these rows by L2 process and function\n  Then the system should store counts for reporting",
    "Acceptance_Criteria": "1. Aggregate shadow headcount by L2 process and function.\n2. Ensure data integrity for reporting.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-082",
    "Epic": "Calculate Shadow Organization Reassignment and Post-Reorganization Headcount",
    "Tool_Process": "Census (old)",
    "Feature": "Calculate Post-Reassignment Headcount",
    "User_Story": "As a user. I want to calculate total headcount after shadow org reassignment So that I can provide accurate post-reorganization figures",
    "Description": null,
    "Scenario": "Scenario: Calculate total headcount after shadow org assignment\n  Given counts for \"In function cost center with matching job nature\" and \"Total shadow headcount\"\n  When the system adds these values for each L2 process\n  Then the system should store the sum as \"Total HC after shadow org assignment\"",
    "Acceptance_Criteria": "1. Total HC after shadow org assignment = matching job nature count + shadow headcount.\n2. Store per L2 process and function.\n3. Log for reporting and output slides.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-083",
    "Epic": "Calculate Lever-Based Headcount Impact Using APQC Benchmarks",
    "Tool_Process": "Census (old)",
    "Feature": "Calculate Impacted and Remaining Headcount Based on Stop Lever and APQC Benchmarks",
    "User_Story": "As a user, I want to list all L2 processes associated with the function being summarized\nSo that they can serve as anchors for headcount and benchmark calculations",
    "Description": null,
    "Scenario": "Scenario: List all L2 processes for the function\n  Given the census file and mappings from previous steps\n  When the system retrieves all L2 processes linked to the function being summarized\n  Then the system should display the list of L2 processes sorted by function",
    "Acceptance_Criteria": "1. System must retrieve all L2 processes mapped to census rows for the function.\n2. Sort processes by function for clarity.\n3. If no L2 processes exist, return an empty list and log the issue.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": "Access to APQC database",
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-084",
    "Epic": "Calculate Lever-Based Headcount Impact Using APQC Benchmarks",
    "Tool_Process": "Census (old)",
    "Feature": "Calculate Impacted and Remaining Headcount Based on Stop Lever and APQC Benchmarks",
    "User_Story": "As a user, I want to match the total headcount after shadow org assignment to each L2 process\nSo that I can use it as the baseline for lever impact calculations",
    "Description": null,
    "Scenario": "Scenario: Match total headcount after shadow org assignment\n  Given the output from process 5a\n  When the system maps headcount values to each L2 process\n  Then the system should store these values for further calculations",
    "Acceptance_Criteria": "1. Each L2 process must have a corresponding headcount value from step 5a.\n2. If headcount is missing for any L2 process, log and set value to 0.\n3. Ensure mapping integrity for downstream calculations.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-085",
    "Epic": "Calculate Lever-Based Headcount Impact Using APQC Benchmarks",
    "Tool_Process": "Census (old)",
    "Feature": "Calculate Impacted and Remaining Headcount Based on Stop Lever and APQC Benchmarks",
    "User_Story": "As a user, I want to retrieve APQC benchmarks for each L2 process\nSo that I can compare actual headcount against industry standards",
    "Description": null,
    "Scenario": "Scenario: Retrieve APQC benchmarks for L2 processes\n  Given an APQC benchmark mapping file\n  When the system matches each L2 process to its benchmark\n  Then the system should retrieve first, second, and third quartile values filtered by revenue and industry",
    "Acceptance_Criteria": "1. System must retrieve Q1, Q2, Q3 benchmarks for each L2 process.\n2. Apply filters for revenue, industry, and other relevant parameters.\n3. If benchmark is missing for any L2 process, log and set default values.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-086",
    "Epic": "Calculate Lever-Based Headcount Impact Using APQC Benchmarks",
    "Tool_Process": "Census (old)",
    "Feature": "Calculate Impacted and Remaining Headcount Based on Stop Lever and APQC Benchmarks",
    "User_Story": "As a user, I want to calculate impacted headcount for each L2 process So that I can determine how many FTEs will be reduced using the Stop lever",
    "Description": null,
    "Scenario": "Scenario 1: Calculate impacted headcount when actual > Q3 benchmark\n  Given total headcount after shadow org assignment and Q3 benchmark for an L2 process\n  When total headcount > Q3 benchmark\n  Then impacted headcount = total headcount - Q3 benchmark\n  And store this value as \"Impacted HC - Stop\"\n\nScenario 2: No impacted headcount when actual ≤ Q3 benchmark\n  Given total headcount after shadow org assignment and Q3 benchmark for an L2 process\n  When total headcount ≤ Q3 benchmark\n  Then impacted headcount = 0\n  And store this value as \"Impacted HC - Stop\"",
    "Acceptance_Criteria": "1. Impacted HC = total HC - Q3 benchmark if total HC > Q3 benchmark.\n2. Impacted HC = 0 if total HC ≤ Q3 benchmark.\n3. Ensure impacted HC ≥ 0 for all cases.\n4. Log calculations for audit.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-087",
    "Epic": "Calculate Lever-Based Headcount Impact Using APQC Benchmarks",
    "Tool_Process": "Census (old)",
    "Feature": "Calculate Impacted and Remaining Headcount Based on Stop Lever and APQC Benchmarks",
    "User_Story": "As a user, I want to calculate remaining headcount after applying the Stop lever\nSo that I can provide accurate post-lever headcount figures",
    "Description": null,
    "Scenario": "Scenario: Calculate remaining headcount after Stop lever\n  Given total headcount after shadow org assignment and impacted headcount\n  When the system subtracts impacted headcount from total headcount\n  Then remaining headcount = total headcount - impacted headcount\n  And store this value for reporting",
    "Acceptance_Criteria": "1. Remaining HC = total HC after shadow org assignment - impacted HC.\n2. Remaining HC must be ≥ 0.\n3. Store values per L2 process for reporting and output slides.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-088",
    "Epic": null,
    "Tool_Process": null,
    "Feature": null,
    "User_Story": null,
    "Description": null,
    "Scenario": null,
    "Acceptance_Criteria": null,
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-089",
    "Epic": null,
    "Tool_Process": null,
    "Feature": null,
    "User_Story": null,
    "Description": null,
    "Scenario": null,
    "Acceptance_Criteria": null,
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-090",
    "Epic": null,
    "Tool_Process": null,
    "Feature": null,
    "User_Story": null,
    "Description": null,
    "Scenario": null,
    "Acceptance_Criteria": null,
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-091",
    "Epic": null,
    "Tool_Process": null,
    "Feature": null,
    "User_Story": null,
    "Description": null,
    "Scenario": null,
    "Acceptance_Criteria": null,
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-092",
    "Epic": null,
    "Tool_Process": null,
    "Feature": null,
    "User_Story": null,
    "Description": null,
    "Scenario": null,
    "Acceptance_Criteria": null,
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-093",
    "Epic": null,
    "Tool_Process": null,
    "Feature": null,
    "User_Story": null,
    "Description": null,
    "Scenario": null,
    "Acceptance_Criteria": null,
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-094",
    "Epic": null,
    "Tool_Process": null,
    "Feature": null,
    "User_Story": null,
    "Description": null,
    "Scenario": null,
    "Acceptance_Criteria": null,
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-095",
    "Epic": null,
    "Tool_Process": null,
    "Feature": null,
    "User_Story": null,
    "Description": null,
    "Scenario": null,
    "Acceptance_Criteria": null,
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-096",
    "Epic": null,
    "Tool_Process": null,
    "Feature": null,
    "User_Story": null,
    "Description": null,
    "Scenario": null,
    "Acceptance_Criteria": null,
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-097",
    "Epic": null,
    "Tool_Process": null,
    "Feature": null,
    "User_Story": null,
    "Description": null,
    "Scenario": null,
    "Acceptance_Criteria": null,
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-098",
    "Epic": null,
    "Tool_Process": null,
    "Feature": null,
    "User_Story": null,
    "Description": null,
    "Scenario": null,
    "Acceptance_Criteria": null,
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-099",
    "Epic": null,
    "Tool_Process": null,
    "Feature": null,
    "User_Story": null,
    "Description": null,
    "Scenario": null,
    "Acceptance_Criteria": null,
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-100",
    "Epic": "Non-HC Analysis",
    "Tool_Process": "Spend for Synergy",
    "Feature": "Spend File Upload (Non-HC Analysis)",
    "User_Story": "As a user, I want to upload PO, GL, Contract, and optional Taxonomy files for Buyer and Target,\nSo that I can perform Spend analysis accurately.",
    "Description": null,
    "Scenario": "Scenario 1: Display upload prompt for Spend\nGiven Non-HC analysis is selected\nWhen the Summary Screen loads\nThen the system should prompt the user to upload:\n  - Buyer PO Data\n  - Target PO Data\n  - Buyer GL Data (optional)\n  - Target GL Data (optional)\n  - Buyer Contract Data\n  - Target Contract Data\n  - Buyer Taxonomy (optional)\n  - Target Taxonomy (optional)\nAnd display instructions and templates for each\n\nScenario 2: Validate Spend files\nGiven the user uploads PO or Contract files\nWhen the system processes the file\nThen the system should check for missing mandatory fields in any row\nAnd mark status as \"Needs Review\" if any missing\n\nScenario 3: Merger vs Tuck-in logic for Spend\nGiven the deal type is Merger of Equals\nThen all Buyer and Target Spend files are mandatory\nElse if deal type is Tuck-in/Acquisition\nThen Target files are mandatory and Buyer files optional\n",
    "Acceptance_Criteria": "1. Required fields for PO Data: Cost Centre, Business Unit, Region, Contract No., Invoice Number, Invoice Date, Currency, Ordered Quantity, Unit Price, Total Amount, Item No., Item Description, Manufacturer Name, Manufacturer Item Code, Vendor Name, PO Number, PO Date, UOM, Conversion Factor, Buying Channel, Categories.\n2. GL Data optional fields listed.\n3. Contract Data required fields listed.\n4. Instructions and templates provided for each file type.\n5. Instructions include:\n         “Ensure all mandatory fields are filled.”\n         “File must be in .xls or .xlsx format.”\n6. Sample template downloadable.\n7. Status indicators: Missing, Uploaded, Needs Review.\n Missing mandatory fields → Needs Review.\n8. Display message: “Some rows have missing data. Update file or override to proceed.”\n9. Override option available with confirmation.\n10. Validation enforced based on deal type.\n11. Error message if mandatory file missing: “You must upload Target Census.xlsx before proceeding. Click here for a template.”",
    "Sprint": 4.0,
    "Effort_Point": 15.0,
    "Effort_Required": 120.0,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-101",
    "Epic": "Identification key fields and standardization",
    "Tool_Process": "Spend",
    "Feature": "Automatic Identification of Key Fields & Standardization",
    "User_Story": "As a user, I want the tool to automatically identify key fields in spend, taxonomy, and contracts export files\nSo that I can quickly map them to normalized column headers without manual effort",
    "Description": null,
    "Scenario": "\nScenario: Auto-detect key fields across supported files\n  Given the user uploads spend, taxonomy, and contracts export files\n  And the system has a library of normalized headers and detection rules\n  When the system scans each file’s headers and samples the data\n  Then the system should propose a mapping of source columns to normalized headers\n  And assign a confidence score to each proposed mapping\n  And flag columns where no confident mapping can be determined\n",
    "Acceptance_Criteria": "1. Detection coverage: Must attempt detection for all uploaded files of types spend, taxonomy, and contracts.\n2. Normalized header library: Uses a predefined schema (e.g., Supplier Name, Supplier ID, Invoice Number, PO Number, Cost Center, GL Code, Category, Subcategory, Contract ID, Contract Start/End, Currency, Country, State, Taxonomy Node, etc.).\n3. Confidence scoring: Each proposed mapping gets a score (0–100).\n              ≥90 = High, 70–89 = Medium, <70 = Low.\n4. Flagging: Any column with Low confidence or no match must be flagged “Needs review”.\n5. Performance: Detection must complete within 60 seconds for files up to 250k rows / 100 columns.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-102",
    "Epic": "Identification key fields and standardization",
    "Tool_Process": "Spend",
    "Feature": "Automatic Identification of Key Fields & Standardization",
    "User_Story": "As a System User\nI want to review the proposed mappings in a UI\nSo that I can approve, reject, or adjust mappings before standardization",
    "Description": null,
    "Scenario": "\nScenario: Present proposed mappings for review and approval\n  Given the system has generated proposed mappings with confidence scores\n  When the user opens the mapping review screen\n  Then the UI should display each source column, proposed normalized header, and confidence level\n  And allow the user to accept, reject, or change the normalized header\n  And allow bulk actions (accept all high confidence, reject all low confidence)\n",
    "Acceptance_Criteria": "1. UI details: Show source column name, sample values, proposed normalized header, confidence score.\n2. Actions: Support Accept, Reject, Change to another normalized header from a searchable dropdown.\n3. Bulk actions: “Accept all High confidence”, “Reject all Low confidence”.\n4. Preview: 20-row sample transformed view per column.\n5. Validation: Prevent duplicate mappings to the same normalized header unless allowed by schema (e.g., multiple attribute columns).\n6. Accessibility: Keyboard navigation + tooltips for confidence explanation.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-103",
    "Epic": "Identification key fields and standardization",
    "Tool_Process": "Spend",
    "Feature": "Automatic Identification of Key Fields & Standardization",
    "User_Story": "As a user, I want to manually map unmapped or incorrectly mapped columns\nSo that the standardized dataset is accurate",
    "Description": null,
    "Scenario": "\nScenario: Manually override a proposed mapping\n  Given a proposed mapping is flagged “Needs review”\n  When the user selects a different normalized header from the dropdown\n  Then the system should update the mapping\n  And revalidate for conflicts\n  And mark the mapping source as “User override”\n",
    "Acceptance_Criteria": "1. Override: Any mapping can be changed; mark with Override flag.\n2. Conflict detection: If the selected normalized header is already used and schema forbids duplicates, show warning and block save.\n3. Audit trail: Log who, when, from (old header) to (new header), and reason (optional note).\n4. Undo/Redo: Provide undo last change and reset to auto-detected options.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-104",
    "Epic": "Identification key fields and standardization",
    "Tool_Process": "Spend",
    "Feature": "Automatic Identification of Key Fields & Standardization",
    "User_Story": "As a System User, I want to save validated mappings as templates\nSo that future files from the same source can be standardized quickly",
    "Description": null,
    "Scenario": "\nScenario: Save validated mapping as a template and reuse\n  Given the user has completed validation of all column mappings\n  When the user clicks “Save mapping as template”\n  Then the system should store the template with source metadata and schema version\n  And allow reloading the template on future uploads\n  And auto-apply the template when source signature matches\n",
    "Acceptance_Criteria": "1. Template save: Persist mappings with source signature (file name pattern, vendor system, column set hash).\n2. Schema versioning: Store schema version to check compatibility on reuse.\n3. Auto-apply: When signature matches ≥90% of columns, auto-apply and highlight deviations.\n4. Fallback: If mismatches exceed threshold, do not auto-apply; require user validation.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-105",
    "Epic": "Identification key fields and standardization",
    "Tool_Process": "Spend",
    "Feature": "Automatic Identification of Key Fields & Standardization",
    "User_Story": "As auser, I want the tool to output standardized datasets\nSo that downstream processes can rely on consistent headers",
    "Description": null,
    "Scenario": "\nScenario: Generate standardized dataset after validation\n  Given all required key fields are mapped and validated\n  When the user clicks “Standardize & Export”\n  Then the system should transform the input files to normalized headers\n  And produce outputs in CSV and Excel formats\n  And include a mapping manifest and validation report\n",
    "Acceptance_Criteria": "1. Required key fields per file type must be present and mapped:\n         Spend: Supplier ID/Name, Invoice/PO, Amount,          Currency, Date, GL/Cost Center, Category/Subcategory.\n        Taxonomy: Category tree (L1–L3+), Node IDs, Descriptions.\n         Contracts: Contract ID, Parties, Start/End, Status, Value, Currency.\n2. Output formats: CSV and Excel (.xlsx) with normalized headers.\n3. Manifests: Include column mapping manifest (source→normalized), confidence summary, and exceptions list.\n4. Data integrity: Row counts must match input; no data loss except explicitly dropped unmapped optional columns (must list).",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-106",
    "Epic": "Identification key fields and standardization",
    "Tool_Process": "Spend",
    "Feature": "Automatic Identification of Key Fields & Standardization",
    "User_Story": "As a Compliance Reviewer\nI want a complete audit trail and validation report\nSo that all mappings are traceable and defensible\nI want clear handling of edge cases\nSo that the tool remains reliable in real-world messy data",
    "Description": null,
    "Scenario": "\nScenario: Generate audit trail for detection and validation\n  Given the user has finalized mappings\n  When the system generates standardized outputs\n  Then it should also produce an audit log containing detection scores, overrides, user actions, timestamps, and schema version\n  And attach the audit log to the export bundle\n",
    "Acceptance_Criteria": "1. Audit log includes: auto-detected mappings, confidence scores, all user overrides, bulk actions, timestamps, user IDs, schema version, and template IDs.\n2. Immutable storage: Write-once log retained with export bundle and in system history.\n3. Filtering: Ability to filter report by file type, confidence band, overridden vs. auto.\n\n",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-107",
    "Epic": "Identification key fields and standardization",
    "Tool_Process": "Spend",
    "Feature": "Automatic Identification of Key Fields & Standardization",
    "User_Story": "As a Developer,  I want an API to submit files and receive proposed mappings\nSo that I can integrate the mapping flow into pipelines",
    "Description": null,
    "Scenario": "\nScenario: Submit files to mapping API and receive proposals\n  Given an authenticated client posts export files to the mapping endpoint\n  When the system processes detection\n  Then it should return proposed mappings with confidence scores and required-field status\n",
    "Acceptance_Criteria": "1. Endpoints: /mapping/propose, /mapping/validate, /mapping/export.\n2. Response: JSON containing proposed mappings, confidence, required-field status, issues.\n3. Errors: Clear error codes for unsupported formats, schema mismatch, auth failures.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-108",
    "Epic": "Data Accuracy and completeness",
    "Tool_Process": "Spend",
    "Feature": "Ensure data accuracy and completeness",
    "User_Story": "As a Data Engineer, I want to clean and load raw data into the system So that it is ready for normalization and mapping",
    "Description": null,
    "Scenario": "\nScenario: Clean and load raw data\n  Given the user uploads raw spend, taxonomy, or contract files\n  When the system validates file format and structure\n  Then the system should remove empty rows, trim whitespace, and standardize date formats\n",
    "Acceptance_Criteria": "\"1. Supported formats: .csv, .xlsx.\n2. Remove empty rows and columns.\n3. Trim leading/trailing spaces from all fields.\n4. Standardize date formats to YYYY-MM-DD.\n5. Validate numeric fields (e.g., amounts) for non-numeric characters.\n6. Log all cleaning actions in an audit report.\"",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-109",
    "Epic": "Data Accuracy and completeness",
    "Tool_Process": "Spend",
    "Feature": "Ensure data accuracy and completeness",
    "User_Story": "As a user,I want the tool to translate foreign language text to English So that all data is standardized for downstream processing",
    "Description": null,
    "Scenario": "\nScenario: Convert foreign language text to English\n  Given the uploaded file contains non-English text in headers or data fields\n  When the system detects foreign language content\n  Then the system should translate text to English using a predefined dictionary or translation API\n  And mark translated fields for review\n",
    "Acceptance_Criteria": "1. Detect language using language identification algorithm.\n2. Translate headers and text fields to English.\n3. Maintain original text in a separate column for reference.\n4. Mark translated fields with a “Translated” flag.\n5. Important: Tool does NOT perform currency conversion (must explicitly state in UI).\n6. Log all translations in the audit report.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-110",
    "Epic": "Data Accuracy and completeness",
    "Tool_Process": "Spend",
    "Feature": "Ensure data accuracy and completeness",
    "User_Story": "As a  user, I want to ingest cleaned and standardized data into an SQL table So that it can be referenced in the backend for normalization and mapping",
    "Description": null,
    "Scenario": "\nScenario: Load cleaned data into SQL table\n  Given the cleaned and standardized data is ready\n  When the system processes ingestion\n  Then the system should create an SQL table with standardized column names\n  And store metadata including source file name, upload timestamp, and schema version\n",
    "Acceptance_Criteria": "1. Create SQL table with normalized column headers.\n2. Preserve original column names in metadata table.\n3. Store source file name, upload timestamp, schema version.\n4. Validate row count matches input file.\n5. Handle large files efficiently (up to 1M rows).\n6. Log ingestion status and any errors.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-111",
    "Epic": "Data Accuracy and completeness",
    "Tool_Process": "Spend (for Synergy)",
    "Feature": "Ensure data accuracy and completeness",
    "User_Story": "As a User, I want to tag buyer and seller data appropriately\nSo that roles are clearly identified for reporting and analysis",
    "Description": null,
    "Scenario": "\nScenario: Tag buyer and seller based on file naming convention\n  Given the uploaded file name contains buyer or seller identifiers\n  When the system processes the file\n  Then the system should add a new column “Entity Role” with value “Buyer” or “Seller”\n  And validate tagging accuracy against naming convention rules\n",
    "Acceptance_Criteria": "1. Detect buyer/seller from file name pattern (e.g., Buyer_Contracts.xlsx, Seller_Spend.csv).\n2. Add column Entity Role with value Buyer or Seller.\n3. If naming convention is ambiguous, flag for manual review.\n4. Allow user override of tagging in UI.\n5. Log tagging decisions in audit report.",
    "Sprint": 4.0,
    "Effort_Point": 10.0,
    "Effort_Required": 80.0,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-112",
    "Epic": "Normalize currency",
    "Tool_Process": "Spend (for Synergy)",
    "Feature": "Normalize Currency Based on Project Settings",
    "User_Story": "As a System, I want to verify if currency normalization is enabled in project settings\nSo that I can decide whether to proceed with currency conversion or skip",
    "Description": null,
    "Scenario": "\nScenario: Skip normalization when toggle is OFF\n  Given the project creation page has a currency normalization toggle\n  When the toggle is set to OFF\n  Then the system should skip currency normalization\n  And proceed to the next step without conversion\n",
    "Acceptance_Criteria": "1. If toggle = OFF → skip all currency conversion logic.\n2. Log decision in audit trail.\n3. Display message: “Currency normalization disabled; original currency retained.”",
    "Sprint": 5.0,
    "Effort_Point": 10.0,
    "Effort_Required": 80.0,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-113",
    "Epic": "Normalize currency",
    "Tool_Process": "Spend (for Synergy)",
    "Feature": "Normalize Currency Based on Project Settings",
    "User_Story": "As a System, I want to retrieve the standardized currency selected by the user\nSo that I can use it as the target currency for normalization",
    "Description": null,
    "Scenario": "\nScenario: Retrieve standardized currency from project settings\n  Given the user has selected a standardized currency (e.g., USD)\n  When the system checks project settings\n  Then the system should store the selected currency as the target for normalization\n  And validate that the selected currency is supported (e.g., USD, EUR, GBP)\n  And if no currency is selected, default to USD and log a w\n",
    "Acceptance_Criteria": "1. Must retrieve selected currency from project configuration.\n2. Validate that selected currency is supported (e.g., USD, EUR, GBP).\n3. If no currency selected, default to USD and log warning.",
    "Sprint": 5.0,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-114",
    "Epic": "Normalize currency",
    "Tool_Process": "Spend (for Synergy)",
    "Feature": "Normalize Currency Based on Project Settings",
    "User_Story": "As a Data Processor, I want to review the currency field in PO, contract, and GL data files\nSo that I can determine the current currency for each record",
    "Description": null,
    "Scenario": "\nScenario: Identify current currency in uploaded files\n  Given PO, contract, and GL data files are uploaded\n  When the system scans the currency field in each file\n  Then the system should identify the current currency for each record\n  And flag any missing or invalid currency codes\n",
    "Acceptance_Criteria": "1. Validate presence of currency field in all files.\n2. Normalize currency codes (e.g., “USD”, “US$”, “Dollar” → “USD”).\n3. If currency field missing → log error and skip conversion for that record.\n4. If invalid currency code → flag for manual review.",
    "Sprint": 5.0,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-115",
    "Epic": "Normalize currency",
    "Tool_Process": "Spend (for Synergy)",
    "Feature": "Normalize Currency Based on Project Settings",
    "User_Story": "As a System, I want to compare current currency with standardized currency\nSo that I can decide whether conversion is needed",
    "Description": null,
    "Scenario": "\nScenario 1: No conversion needed when currencies match\n  Given the current currency matches the standardized currency\n  When the system processes the record\n  Then no conversion should be applied\n  And costs should remain unchanged\n\n Scenario 2: Given the current currency differs from the standardized currency\n  When the system processes the record\n  Then the system should call the Open AI websearch API to retrieve the latest conversion rate\n  And apply the conversion rate to the following fields:\n    | Unit Price     |\n    | Total Amount   |\n    | Debit (if not blank) |\n    | Credit (if not blank) |\n  And update these fields to reflect the standardized currency\n  And store the original values in separate columns for audit purposes\n  And log the conversion details including:\n    | Original Currency |\n    | Target Currency   |\n    | Conversion Rate   |\n    | Timestamp         |\n",
    "Acceptance_Criteria": "API Integration:\n    Must call Open AI web search API for real-time conversion rate.\n    Retry up to 3 times if API fails; fallback to last known rate if retries fail.\nConversion Logic:\n      Apply conversion to Unit Price, Total Amount, Debit, and Credit fields.\n     Ensure precision up to 2 decimal places.\nAudit & Traceability:\n     Store original values in separate columns.\n     Log conversion details (original currency, target currency, rate, timestamp).\nEdge Cases:\n      If any financial field is blank, skip conversion for that field.\n      If API unavailable, flag record for manual review.",
    "Sprint": 5.0,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": "High",
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-116",
    "Epic": "Harmonize vendor and manufacturer names",
    "Tool_Process": "Spend",
    "Feature": "Standardize Supplier Names and Create Harmonized Master List",
    "User_Story": "As a user I want to standardize supplier names to avoid duplicates and variations So that reporting and analytics are consistent across all datasets",
    "Description": null,
    "Scenario": "\nScenario: Standardize supplier names to avoid duplicates\n  Given the system has a list of supplier names from uploaded files\n  When the system detects variations of the same supplier (e.g., \"ABC Corp\" vs \"ABC Corporation\")\n  Then the system should normalize these names to a single standardized name\n",
    "Acceptance_Criteria": "1. Detect variations using:\n           String similarity (e.g., Levenshtein distance).\n            Common abbreviations (e.g., Corp → Corporation, Ltd → Limited).\n            Case-insensitive comparison.\n2. Normalize names to a single canonical form.\n3. Maintain a mapping table of original → standardized names.\n4. If confidence < threshold (e.g., 85%), flag for manual review.\n5. Log all changes for audit.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-117",
    "Epic": "Harmonize vendor and manufacturer names",
    "Tool_Process": "Spend",
    "Feature": "Standardize Supplier Names and Create Harmonized Master List",
    "User_Story": "As a User,I want to review and override standardized supplier names So that I can ensure accuracy in cases where automation is uncertain",
    "Description": null,
    "Scenario": "\nScenario: Manual override of standardized supplier name\n  Given the system has proposed standardized names with confidence scores\n  When the user reviews the mapping\n  Then the user should be able to accept, reject, or override the standardized name\n",
    "Acceptance_Criteria": "1. Display original name, proposed standardized name, confidence score.\n2. Allow user to override with a custom name.\n3. Mark overridden mappings with an “Override” flag.\n4. Log user actions (who, when, old value, new value).\n5. Provide bulk approval for high-confidence matches.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-118",
    "Epic": "Harmonize vendor and manufacturer names",
    "Tool_Process": "Spend",
    "Feature": "Standardize Supplier Names and Create Harmonized Master List",
    "User_Story": "As a user, I want to create a master list of harmonized supplier names So that it can feed into the golden database for future projects",
    "Description": null,
    "Scenario": "\nScenario: Generate harmonized master supplier list\n  Given all supplier names have been standardized and validated\n  When the system compiles the final list\n  Then the system should create a master list of unique standardized supplier names\n  And store it in the golden database for reuse\n",
    "Acceptance_Criteria": "1. Master list must contain unique standardized names only.\n2. Include metadata: source files, date of creation, version.\n3. Store in golden database for future projects.\n4. Validate against existing master list to prevent duplicates.\n5. Export as CSV and Excel for reporting.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-119",
    "Epic": "Seek Supplier parent company",
    "Tool_Process": "Spend",
    "Feature": "Seek Supplier Parent Company via LLM Intelligence",
    "User_Story": "As a user,I want the system to construct a custom prompt and query an LLM to identify supplier parent companies So that I can enrich supplier records with parent relationships",
    "Description": null,
    "Scenario": "\nScenario: Construct prompt and query LLM for parent company\n  Given a list of standardized supplier names with optional metadata (country, URL, registry ID)\n  And an approved prompt template is configured\n  When the system generates a supplier-specific prompt including name and available metadata\n  And sends the prompt to the LLM\n  Then the system should receive a structured response containing:\n    | parent_company_name | confidence | sources | notes |\n  And validate the response format against the schema\n",
    "Acceptance_Criteria": "1. Prompt template must include: supplier name, country (if available), industry (if available), “return JSON” instruction, and do not invent data caveat.\n2. Response schema must be strictly validated; malformed responses are rejected and retried (max 2 retries).\n3. Timeouts: LLM call must complete within 30 seconds per batch.\n4. Rate limits handled via queue/throttling (configurable).",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-120",
    "Epic": "Seek Supplier parent company",
    "Tool_Process": "Spend",
    "Feature": "Seek Supplier Parent Company via LLM Intelligence",
    "User_Story": "As a user, I want to normalize LLM-returned parent company names So that duplicates and variations map to a single canonical parent record",
    "Description": null,
    "Scenario": "\nScenario: Canonicalize parent company names\n  Given LLM responses with parent_company_name values\n  When the system applies normalization rules\n  Then it should map variations (e.g., \"ABC Holdings Inc.\" vs \"ABC Holdings\") to a single canonical parent\n  And persist the original and canonical forms in a mapping table\n",
    "Acceptance_Criteria": "1. Apply standardization rules (case folding, punctuation removal, legal suffix expansion e.g., Inc/Corp/Ltd).\n2. Use fuzzy matching (threshold configurable; default ≥ 0.90) + alias dictionary.\n3. Create parent_canonical_id; store original_name → canonical_name with match_score and method (rule/fuzzy/alias).\n4. Conflicts (two distinct canonicals within threshold) → flag for review.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-121",
    "Epic": "Seek Supplier parent company",
    "Tool_Process": "Spend",
    "Feature": "Seek Supplier Parent Company via LLM Intelligence",
    "User_Story": "As a user, I want confidence bands with a human-in-the-loop review So that uncertain parent mappings are validated before use",
    "Description": null,
    "Scenario": "\nScenario: Apply confidence bands and route to review\n  Given a parent mapping result has confidence <threshold>\n  When the system evaluates confidence against bands\n  Then the result should be <routing>\n  And the UI should display original supplier, proposed parent, score, and sources\n",
    "Acceptance_Criteria": "1. High (≥0.90) → auto-approve; no reviewer needed.\n2. Medium (0.70–0.89) → routed to review queue; reviewer can approve/reject/override.\n3. Low (<0.70) → blocking; must be manually resolved.\n4. Reviewer actions are fully audited (user, time, old/new value, reason).",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-122",
    "Epic": "Seek Supplier parent company",
    "Tool_Process": "Spend",
    "Feature": "Seek Supplier Parent Company via LLM Intelligence",
    "User_Story": "As a user, I want the LLM output to include public references/sources so that parent relationships are traceable and defensible",
    "Description": null,
    "Scenario": "\nSenario: Require sources in LLM output and store for audit\n  Given an LLM response includes a list of sources (URLs or registry identifiers)\n  When the system validates and stores the response\n  Then it should persist the sources alongside the parent mapping\n  And include sources in audit/export bundles\n",
    "Acceptance_Criteria": "1. At least one verifiable source (e.g., company registry, investor relations page, reputable database).\n2. If sources are missing → mark Needs verification.\n3. Sources and notes stored with parent_canonical_id, supplier_id.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-123",
    "Epic": "Seek Supplier parent company",
    "Tool_Process": "Spend",
    "Feature": "Seek Supplier Parent Company via LLM Intelligence",
    "User_Story": "As a user, I want to persist the validated supplier→parent mappings So that they feed a golden database for future consistency",
    "Description": null,
    "Scenario": "\nScenario: Persist validated mappings to golden database\n  Given parent mappings are approved (auto or manual)\n  When the system writes to storage\n  Then it should upsert into the golden database:\n    | supplier_canonical_id | parent_canonical_id | confidence | sources | version |\n  And maintain history with effective_start and effective_end dates\n",
    "Acceptance_Criteria": "1. Update existing mapping; retain history via SCD2 (slowly changing dimensions).\n2. Versioning and effective dating required.\n3. Reject writes if parent_canonical_id missing.\n4. Export CSV/Excel + JSON manifest for downstream use.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-124",
    "Epic": "Seek Supplier parent company",
    "Tool_Process": "Spend",
    "Feature": "Seek Supplier Parent Company via LLM Intelligence",
    "User_Story": "As a user, I want caching of prior parent lookups\nSo that repeated suppliers do not trigger new LLM calls",
    "Description": null,
    "Scenario": "\nScenario: Reuse cached parent mapping when supplier recurs\n  Given a supplier_canonical_id already has a validated parent_canonical_id\n  When a new dataset includes the same supplier\n  Then the system should reuse the cached mapping\n  And skip LLM calls unless a “force refresh” flag is set\n",
    "Acceptance_Criteria": "1. Cache keyed by supplier_canonical_id + context (country/industry) when relevant.\n2. Force refresh supported per supplier or batch.\n3. Cache hit/miss stats reported; default TTL = 180 days (configurable).",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-125",
    "Epic": "Seek Supplier parent company",
    "Tool_Process": "Spend",
    "Feature": "Seek Supplier Parent Company via LLM Intelligence",
    "User_Story": "As a user, I want the system to prevent sending PII or sensitive contract data to the LLM So that privacy and compliance are maintained",
    "Description": null,
    "Scenario": "\nScenario: Strip PII and sensitive fields before LLM calls\n  Given supplier records include contact or contract details\n  When the system prepares the LLM prompt\n  Then it should exclude any PII and sensitive fields\n  And only send supplier name and public metadata\n",
    "Acceptance_Criteria": "1. Redaction policy enforced: no emails, phone numbers, addresses, contract values, account numbers.\n2. Allowlist of fields permitted in prompts (name, country, website).\n3. Security logs record prompt payload hashes; raw prompts not stored.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-126",
    "Epic": "Seek Supplier parent company",
    "Tool_Process": "Spend",
    "Feature": "Seek Supplier Parent Company via LLM Intelligence",
    "User_Story": "As a Developer, I want resilient error handling\nSo that processing continues even if LLM calls fail",
    "Description": null,
    "Scenario": "\nScenario: Handle LLM errors and fall back gracefully\n  Given an LLM call results in <error_type>\n  When the system processes the response\n  Then it should <fallback_behavior>\n  And log the error with correlation ID\n\nExamples:\n  | error_type      | fallback_behavior                        |\n  | timeout         | retry with exponential backoff (max 3)   |\n  | 429 rate-limit  | queue and postpone                       |\n  | malformed JSON  | re-prompt with stricter schema           |\n  | empty response  | mark as Unresolved; route to review      |\n",
    "Acceptance_Criteria": "1. Retries: exponential backoff, max 3; after failure → Unresolved status.\n2. All errors logged with correlation ID, timestamp, supplier_id.\n3. Batch continues; failed items are isolated.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-127",
    "Epic": "Seek Supplier parent company",
    "Tool_Process": "Spend",
    "Feature": "Seek Supplier Parent Company via LLM Intelligence",
    "User_Story": "As a Reviewer, I want an interface to triage uncertain mappings\nSo that I can correct parent assignments quickly",
    "Description": null,
    "Scenario": "\nScenario: Review and override uncertain mappings\n  Given a queue of Medium/Low confidence mappings\n  When the reviewer opens an item\n  Then the UI should show supplier details, proposed parent, confidence, sources\n  And allow Approve, Reject, Override (search canonical parent), or Request more evidence\n",
    "Acceptance_Criteria": "1. Search canonical parents; create new canonical parent if none exists.\n2. Bulk actions for Medium-band items.\n3. Comments required on Reject/Override.\n4. Changes reflected immediately and audit logged.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-128",
    "Epic": "Seek Supplier parent company",
    "Tool_Process": "Spend",
    "Feature": "Seek Supplier Parent Company via LLM Intelligence",
    "User_Story": "As a User, I want reports of supplier→parent coverage and confidence So that I understand data quality and next actions",
    "Description": null,
    "Scenario": "\nScenario: Produce coverage and quality report\n  Given the processing run has completed\n  When the system generates a report\n  Then it should include:\n    | total suppliers | mapped suppliers | auto-approved | human-approved | unresolved |\n    | confidence distribution | top sources used | overrides count |\n  And export as CSV/Excel and a PDF summary\n",
    "Acceptance_Criteria": "1. Must include coverage metrics, confidence histogram, review backlog size.\n2. Drill-down by industry/country (if available).\n3. Retain reports with run ID and date.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-129",
    "Epic": "Define Taxonomy",
    "Tool_Process": "Spend",
    "Feature": "Define Client Taxonomy & Map Using LLM (Primary Reference)",
    "User_Story": "As a user, I want to use the client’s uploaded taxonomy as the primary reference So that all subsequent mappings conform to the client’s structure",
    "Description": null,
    "Scenario": "\nScenario: Load client taxonomy and set as primary reference\n  Given the client has uploaded a taxonomy file in step 1a\n  When the system validates and loads the taxonomy\n  Then the system should mark the client taxonomy as \"Primary Reference\"\n  And store its schema, version, and effective date\n",
    "Acceptance_Criteria": "1. Supported formats: .csv, .xlsx, .json.\n2. Primary flag: Set a system-level flag that points to the client taxonomy (primary).\n3. Metadata stored: filename, version, upload timestamp, source, schema hash.\n4. If no client taxonomy is provided, do not set primary; log and prompt to upload (no silent fallback to EY taxonomy).\n5. Re-uploads with same schema hash will not duplicate; they update version with proper effective dating.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-130",
    "Epic": "Define Taxonomy",
    "Tool_Process": "Spend",
    "Feature": "Define Client Taxonomy & Map Using LLM (Primary Reference)",
    "User_Story": "As a user, I want to validate the uploaded taxonomy\nSo that the hierarchy and identifiers are reliable for mapping",
    "Description": null,
    "Scenario": "\nScenario: Validate taxonomy schema and hierarchical integrity\n  Given a client taxonomy with levels (e.g., L1, L2, L3) and codes\n  When the system runs schema and integrity checks\n  Then it should confirm:\n    And level completeness (no missing required levels)\n    And uniqueness of codes per level\n    And valid parent-child links (no cycles, no orphans)\n    And consistent naming conventions\n  And produce a validation report with errors and warnings\n",
    "Acceptance_Criteria": "1. Hierarchy checks: No cycles; all children have valid parents; no dangling nodes.\n2. Uniqueness: Codes/IDs unique per level; names can differ but flagged if exact duplicates exist within level.\n3. Conventions: Code formats (regex), max length, allowed characters.\n4. Multi-language labels: Detect non-English labels; normalize or flag.\n5. Blocking vs. non-blocking: Critical failures block primary flag; warnings allow continuation but must be logged.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-131",
    "Epic": "Define Taxonomy",
    "Tool_Process": "Spend",
    "Feature": "Define Client Taxonomy & Map Using LLM (Primary Reference)",
    "User_Story": "As a user, I want to use LLM-intelligence to map into the client taxonomy format So that our internal taxonomy aligns to the client’s reference structure",
    "Description": null,
    "Scenario": "\nScenario: Construct prompt and map EY taxonomy nodes to client taxonomy\n  Given the client taxonomy is set as Primary Reference\n  And an approved prompt template is configured\n  When the system generates prompts for each EY node (with node name, description, examples)\n  And sends them to the LLM\n  Then the system should receive a structured response containing:\n    | client_level | client_code | client_name | confidence | rationale |\n  And validate the response against the schema\n",
    "Acceptance_Criteria": "1. Prompt template includes: EY node name, definition/description, sample activities, instruction to return strict JSON, “do not invent” caveat.\n2. Response validation: Reject malformed responses and retry up to 2 times with stricter schema.\n3. Confidence scoring: Numeric [0–1] or [0–100]; categorized High (≥0.9), Medium (0.7–0.89), Low (<0.7).\n4. PII safety: Only taxonomy text; no transactional or sensitive data in prompts.\n5. Latency controls: Batch rate-limited; per-call timeout ≤30 seconds.\n\n",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-132",
    "Epic": "Define Taxonomy",
    "Tool_Process": "Spend",
    "Feature": "Define Client Taxonomy & Map Using LLM (Primary Reference)",
    "User_Story": "As a user I want consistency checks after LLM mapping\nSo that mapped EY nodes align cleanly with the client’s taxonomy levels and codes",
    "Description": null,
    "Scenario": "\nScenario: Validate mapping consistency and coverage\n  Given LLM mappings for EY nodes to client nodes\n  When the system cross-validates level alignment and code existence in the client taxonomy\n  Then it should confirm no level misalignment, code mismatches, or duplicates\n  And produce a coverage report indicating mapped vs. unmapped nodes\n",
    "Acceptance_Criteria": "1. Level alignment: EY L1→Client L1, EY L2→Client L2 (configurable rules supported).\n2. Code existence: Returned client_code must exist in Primary taxonomy.\n3. Duplicates: An EY node must not map to multiple distinct client nodes unless rules allow (one-to-many flagged and routed to review).\n4. Coverage: % of EY nodes mapped ≥ defined threshold (e.g., 95%) or escalate.\n5. Idempotency: Re-running mapping on same inputs yields no drift (unless client taxonomy changed version).",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-133",
    "Epic": "Define Taxonomy",
    "Tool_Process": "Spend",
    "Feature": "Define Client Taxonomy & Map Using LLM (Primary Reference)",
    "User_Story": "As a user I want to review low/medium confidence mappings\nSo that final mappings are accurate and defensible",
    "Description": null,
    "Scenario": "\nScenario: Triage and override uncertain LLM mappings\n  Given mappings with confidence below threshold\n  When the reviewer opens the mapping queue\n  Then the UI should display EY node, proposed client node, confidence, rationale\n  And allow Approve, Reject, or Override (search client taxonomy)\n  And require comments on Reject/Override\n",
    "Acceptance_Criteria": "1. Thresholds: High auto-approve; Medium → queue; Low → mandatory manual resolution.\n2. Override search: Search client taxonomy by name/code; select correct node.\n3. Bulk actions: Approve all High, triage Medium with pagination.\n4. Audit: Record user, timestamp, old/new mapping, reasons.\n5. Accessibility: Keyboard navigation, tooltips for confidence/rationale.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-134",
    "Epic": "Define Taxonomy",
    "Tool_Process": "Spend",
    "Feature": "Define Client Taxonomy & Map Using LLM (Primary Reference)",
    "User_Story": "As a Data Engineer\nI want to persist validated mappings and versions\nSo that they can be reused and audited over time",
    "Description": null,
    "Scenario": "\nScenario: Save mapping dictionary and maintain history\n  Given mappings are approved (auto or manual)\n  When the system writes to storage\n  Then it should upsert a mapping dictionary with:\n    | ey_node_id | client_node_id | confidence | source (LLM/User) | primary_version |\n",
    "Acceptance_Criteria": "1. Upsert logic with SCD2 effective dating.\n2. Referential integrity: client_node_id must exist in 3. 3. Primary taxonomy (current version).\n4. Rollback: Support rollback to prior version.\n5. Export: Provide CSV/XLSX + JSON manifest for downstream flows.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-135",
    "Epic": "Define Taxonomy",
    "Tool_Process": "Spend",
    "Feature": "Define Client Taxonomy & Map Using LLM (Primary Reference)",
    "User_Story": "As a User I want a report summarizing mapping completeness and issues\nSo that I can track readiness and gaps",
    "Description": null,
    "Scenario": "\nScenario: Produce mapping coverage and quality report\n  Given a mapping run has completed\n  When the system generates the report\n  Then it should include:\n    | total EY nodes | mapped | unmapped | auto-approved | user-overridden |\n    | confidence distribution | level alignment issues | duplicates |\n  And store the report with run ID and timestamp\n",
    "Acceptance_Criteria": "1. KPIs: Coverage %, # of issues, # of overrides, distribution by levels.\n2. Filters: By level (L1/L2/L3), by function, by confidence band.\n3. Retention: Reports persisted with run IDs.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-136",
    "Epic": "Define Taxonomy",
    "Tool_Process": "Spend",
    "Feature": "Define Client Taxonomy & Map Using LLM (Primary Reference)",
    "User_Story": "As a user I want resilient processing and caching\nSo that mapping is repeatable and cost-effective",
    "Description": null,
    "Scenario": "\nScenario: Handle LLM and data errors with graceful fallbacks\n  Given a mapping operation encounters <error_type>\n  When the system processes the error\n  Then it should <fallback_behavior>\n  And log the error with correlation ID\n\nExamples:\n  | error_type        | fallback_behavior                           |\n  | LLM timeout       | retry with backoff (max 3) then queue review |\n  | rate-limit (429)  | throttle and postpone batch                  |\n  | malformed response| re-prompt with stricter schema               |\n  | missing client code | mark unmapped and route to reviewer        |\n",
    "Acceptance_Criteria": "1. Retries with exponential backoff; max 3.\n2. Queueing for rate-limit or persistent failures.\n3. Cache previous mappings keyed by EY node + client taxonomy version to avoid redundant LLM calls.\n4. Observability: Metrics for hits/misses, retries, failures.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-137",
    "Epic": "Produce client deliverables as direct outputs",
    "Tool_Process": "Output Generation",
    "Feature": "Generate Client Deliverables and Output Summary Tables",
    "User_Story": "As a user, I want to generate a cleansed and categorized census file So that the client receives a standardized and accurate dataset",
    "Description": null,
    "Scenario": "\nScenario: Generate cleansed and categorized census file\n  Given the census data has been processed through all normalization and mapping steps\n  When the system prepares the output\n  Then the system should produce a cleansed census file with standardized columns\n  And include mapped EY L1, EY L2, client function, and job nature function\n",
    "Acceptance_Criteria": "1. Remove duplicates and invalid rows.\n2. Include standardized columns:\n            Employee ID, Name, Normalized EY L1, EY L2,      Client Function (EY L1 equivalent), Job Nature Function.\n3. Ensure row count matches input (except removed invalid rows).\n4. Export in CSV and Excel (.xlsx) formats.\n5. Attach audit log of transformations.",
    "Sprint": 6.0,
    "Effort_Point": 8.0,
    "Effort_Required": 64.0,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-138",
    "Epic": "Produce client deliverables as direct outputs",
    "Tool_Process": "Output Generation",
    "Feature": "Generate Client Deliverables and Output Summary Tables",
    "User_Story": "As a User I want a summary table grouping EY L2 processes under EY L1 So that I can view process distribution by EY hierarchy",
    "Description": null,
    "Scenario": "\nScenario: Generate summary table grouping EY L2 by EY L1\n  Given all EY L1 and EY L2 mappings are complete\n  When the system generates the summary\n  Then the table should display EY L1 as parent rows and EY L2 processes grouped under each EY L1\n  And include headcount counts for each EY L2 process\n",
    "Acceptance_Criteria": "1. Table format:\n       Columns: EY L1 | EY L2 | Headcount | % of Total.\n2. Sort EY L1 alphabetically; EY L2 grouped under respective EY L1.\n3. Include totals per EY L1 and overall total.\n4. Export as Excel .\n\n",
    "Sprint": 6.0,
    "Effort_Point": null,
    "Effort_Required": 80.0,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-139",
    "Epic": "Produce client deliverables as direct outputs",
    "Tool_Process": "Output Generation",
    "Feature": "Generate Client Deliverables and Output Summary Tables",
    "User_Story": "As a User I want a summary table grouping EY L2 processes under client function (EY L1 equivalent)\nSo that I can view process distribution by client’s organizational structure",
    "Description": null,
    "Scenario": "\nScenario: Generate summary table grouping EY L2 by client function\n  Given client function mappings are complete\n  When the system generates the summary\n  Then the table should display client function as parent rows and EY L2 processes grouped under each client function\n  And include headcount counts for each EY L2 process\n",
    "Acceptance_Criteria": "1. Table format:\n          Columns: Client Function | EY L2 | Headcount | % of Total.\n2. Sort client functions alphabetically.\n3. Include totals per client function and overall total.\n4. Export as Excel .",
    "Sprint": 6.0,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-140",
    "Epic": "Produce client deliverables as direct outputs",
    "Tool_Process": "Output Generation",
    "Feature": "Generate Client Deliverables and Output Summary Tables",
    "User_Story": "As a  User I want a summary table showing EY L1 to EY L2 mapping\nSo that I can validate hierarchical consistency",
    "Description": null,
    "Scenario": "\nScenario: Generate EY L1 to EY L2 mapping table\n  Given EY taxonomy mapping is complete\n  When the system generates the mapping table\n  Then the table should list EY L1 and corresponding EY L2 processes\n  And include headcount counts for each EY L2 process\n",
    "Acceptance_Criteria": "1. Columns: EY L1 | EY L2 | Headcount.\n2. Include totals per EY L1.\n3. Export as Excel .",
    "Sprint": 6.0,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-141",
    "Epic": "Produce client deliverables as direct outputs",
    "Tool_Process": "Output Generation",
    "Feature": "Generate Client Deliverables and Output Summary Tables",
    "User_Story": "As a  User I want a summary table showing client function to EY L2 mapping So that I can validate alignment between client structure and EY taxonomy",
    "Description": null,
    "Scenario": "\nScenario: Generate client function to EY L2 mapping table\n  Given client function mappings are complete\n  When the system generates the mapping table\n  Then the table should list client function and corresponding EY L2 processes\n  And include headcount counts for each EY L2 process\n",
    "Acceptance_Criteria": "1. Columns: Client Function | EY L2 | Headcount.\n2. Include totals per client function.\n3. Export as Excel\n\n",
    "Sprint": 6.0,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-142",
    "Epic": "Consolidated Dashboard",
    "Tool_Process": "Output Generation",
    "Feature": "Provide a consolidated dashboard container",
    "User_Story": "As a User I want a single dashboard within the tool that aggregates spend & compliance metrics\nSo that I can monitor performance and identify opportunities quickly\n",
    "Description": null,
    "Scenario": "\nScenario: Render consolidated dashboard shell\n  Given the user navigates to the Dashboard module\n  When the system loads the default view\n  Then the dashboard should display tiles for key KPIs and charts for Census spend & compliance\n  And provide filter controls for period, BU, category, supplier, currency, and region\n",
    "Acceptance_Criteria": "1. Visible sections: KPIs, Spend Distribution, Vendor Coverage & Tail, Maverick Spend, Seasonality & Trends, Contract Compliance.\n2. Filters: Date range, BU, category hierarchy, supplier, region/country, currency (view-only; no conversion performed here).\n3. Data-refresh timestamp and environment (e.g., “Prod”).\n4. Empty-state messages for filters that yield no data.",
    "Sprint": 6.0,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-143",
    "Epic": "Consolidated Dashboard",
    "Tool_Process": "Output Generation",
    "Feature": "Total spend by BU/category/supplier visualizations",
    "User_Story": "As a  User.,  want charts for total spend by BU, category, and supplier So that I can see distribution and top contributors",
    "Description": null,
    "Scenario": "\nScenario: Show total spend by BU, category, and supplier\n  Given spend data is available for the selected period and filters\n  When the system aggregates spend by BU, category, and supplier\n  Then it should render:\n    And a bar chart for spend by BU\n    And a treemap or stacked bar for spend by category hierarchy\n    And a Pareto (ranked bar) for spend by supplier\n  And totals across all visuals must equal the filtered dataset’s total spend\n",
    "Acceptance_Criteria": "1. Aggregation rules:\n         Spend = sum of approved invoices/POs in filtered period.\n        Exclude canceled/voided transactions.\n2. Category hierarchy: L1→L2→L3 drilldown enabled.\n3. Totals across visuals must equal the filtered total spend (± rounding tolerance ≤ 0.01%).\n4. Tooltips show value, % of total, count of transactions.\n5. Export underlying aggregated tables to CSV/XLSX.",
    "Sprint": 6.0,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-144",
    "Epic": "Consolidated Dashboard",
    "Tool_Process": "Output Generation",
    "Feature": "Total spend by BU/category/supplier visualizations",
    "User_Story": "As a User, I want vendor count metrics and identification of vendors covering top 80% of spend\nSo that I can focus on strategic suppliers and manage the tail",
    "Description": null,
    "Scenario": "\nScenario: Compute vendor counts and top-80% coverage list\n  Given supplier-level spend is aggregated\n  When the system ranks suppliers by spend and computes cumulative percentages\n  Then it should label suppliers contributing up to 80% of total spend as “Top Coverage”\n  And compute:\n    And total vendor count\n    And count of coverage vendors\n    And tail vendor count = total vendors - coverage vendors\n  And display a table of coverage suppliers with spend and % cumulative\n",
    "Acceptance_Criteria": "1. Sorting: Descending by spend; cumulative % recalculated after filters applied.\n2. Totals must reconcile: coverage + tail = total vendor count.\n3. Edge case: If a single supplier >80%, coverage list contains 1; tail = total - 1.\n4. Provide CSV/XLSX export of coverage list.",
    "Sprint": 7.0,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-145",
    "Epic": "Consolidated Dashboard",
    "Tool_Process": "Output Generation",
    "Feature": "Total spend by BU/category/supplier visualizations",
    "User_Story": "As a user, I want a view of tail vendors\nSo that I can target consolidation opportunities",
    "Description": null,
    "Scenario": "\nScenario: Visualize tail vendors\n  Given vendor coverage metrics are computed\n  When the system isolates tail vendors (outside the top 80%)\n  Then it should render:\n    And a distribution chart (e.g., histogram) of tail vendor spend\n    And a table of top 20 tail vendors by spend\n",
    "Acceptance_Criteria": "1. Tail definition: Outside top-80% coverage list.\n2. Chart bins configurable (default 10 bins).\n3. Provide filters for minimum spend thresholds to hide negligible entries.",
    "Sprint": 7.0,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-146",
    "Epic": "Consolidated Dashboard",
    "Tool_Process": "Output Generation",
    "Feature": "Total spend by BU/category/supplier visualizations",
    "User_Story": "As a user, I want % maverick spend\nSo that I can measure off-contract/off-policy buying",
    "Description": null,
    "Scenario": "\nScenario: Calculate % maverick spend\n  Given contract linkage and policy flags are present\n  When the system classifies transactions as compliant vs. non-compliant\n  Then it should compute:\n    And maverick spend = sum(non-compliant spend)\n    And % maverick = maverick spend / total spend * 100\n  And display a KPI and a breakdown by BU and category\n",
    "Acceptance_Criteria": "1. Compliant criteria (configurable): Has valid contract ID, approved supplier, correct category, policy flag not violated.\n2. Non-compliant (maverick): Missing/invalid contract or policy violations.\n3. % values must be accurate and totals for compliant + non-compliant = 100% (± rounding tolerance ≤ 0.01%).\n4. Drill to transaction-level listing for audit.",
    "Sprint": 7.0,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-147",
    "Epic": "Consolidated Dashboard",
    "Tool_Process": "Output Generation",
    "Feature": "Total spend by BU/category/supplier visualizations",
    "User_Story": "As a User, I want monthly/quarterly trend charts\nSo that I can understand seasonality and momentum",
    "Description": null,
    "Scenario": "\nScenario: Display seasonality and trends\n  Given time-series spend data for the selected period\n  When the system aggregates monthly and quarterly totals\n  Then it should render:\n    And a line chart for monthly spend trend\n    And a seasonal decomposition view (YoY comparison if available)\n",
    "Acceptance_Criteria": "1. Support: Monthly and quarterly groupings; show YoY comparison when prior-year data exists.\n2. Handle missing months: Display zero bars; no interpolation.\n3. Tooltips: Month, spend, % change vs prior period.",
    "Sprint": 7.0,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-148",
    "Epic": "Consolidated Dashboard",
    "Tool_Process": "Output Generation",
    "Feature": "Contract compliance dashboard section",
    "User_Story": "As a user, I want contract compliance rates and drilldowns\nSo that I can identify gaps by BU/category/supplier",
    "Description": null,
    "Scenario": "\nScenario: Show contract compliance metrics and breakdowns\n  Given contract metadata is linked to spend transactions\n  When the system computes compliance per BU, category, and supplier\n  Then it should render:\n    And a KPI tile for overall compliance %\n    And bar charts by BU and category\n    And a table of non-compliant suppliers with reasons\n",
    "Acceptance_Criteria": "1. Compliance % = compliant spend / total spend * 100.\n2. Reasons dimension: missing contract, expired contract, supplier not approved, pricing variance beyond threshold.\n3. Totals by dimension must add up to 100% by construction (compliant + non-compliant).\n4. Export non-compliant supplier table.",
    "Sprint": 7.0,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-149",
    "Epic": "Consolidated Dashboard",
    "Tool_Process": "Output Generation",
    "Feature": "Accuracy checks, totals = 100%, and formatting consistency",
    "User_Story": "As a User, I want robust validation rules\nSo that metrics are accurate, reconcilable, and consistently formatted\n",
    "Description": null,
    "Scenario": "\nScenario: Validate dashboard accuracy and formatting\n  Given all metrics are computed\n  When the system runs validation\n  Then totals for stacked/percentage visuals must equal 100% (±0.01%)\n  And total spend across sections must reconcile with the filtered dataset\n  And numeric formatting must follow:\n    | Currency: symbol + thousand separators + 2 decimals |\n    | Percentages: 1 decimal place (e.g., 12.3%)          |\n    | Dates: YYYY-MM-DD                                   |\n",
    "Acceptance_Criteria": "1. Reconciliation: Top-level total spend = sum of BU totals = sum of category totals = sum of supplier totals (within rounding).\n2. Visual checks: stacked charts (e.g., compliant/non-compliant) sum to 100%.\n3. Formatting consistency across tiles, charts, and tables.\n4. Validation errors block publication and display an error banner.",
    "Sprint": 7.0,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-150",
    "Epic": "Consolidated Dashboard",
    "Tool_Process": "Output Generation",
    "Feature": "Interactivity (filters, drill-down, cross-highlighting)",
    "User_Story": "As a  User I want filters and interactive drill-down\nSo that I can explore data and root causes",
    "Description": null,
    "Scenario": "\nScenario: Provide filters, drill-downs, and cross-highlighting\n  Given the user selects a BU/category/supplier in any chart\n  When the selection is applied\n  Then all visuals must cross-filter to the selection\n  And drill-down from category L1→L2→L3 must be supported\n  And clicking a KPI navigates to the relevant detail view\n",
    "Acceptance_Criteria": "1. Cross-filtering is consistent across all visuals in <500ms for datasets ≤1M rows.\n2. Reset filters button returns to default state.\n3. Persist filter state in URL (deep links) or session.",
    "Sprint": 7.0,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-151",
    "Epic": "Consolidated Dashboard",
    "Tool_Process": "Output Generation",
    "Feature": "Performance and stability",
    "User_Story": "As a user, I want the dashboard to load and interact smoothly\nSo that users can rely on it during analysis\n",
    "Description": null,
    "Scenario": "\nScenario: Meet performance SLAs\n  Given a dataset up to 1M transactions and 10k suppliers\n  When the dashboard loads\n  Then initial load must complete within 2 minutes\n  And interactions (filter, sort, drill) must respond within 500ms\n",
    "Acceptance_Criteria": "1. Pre-aggregations/caches enabled.\n2. Lazy loading for tables; pagination for long lists.\n3. Error handling with retry for transient failures.",
    "Sprint": 7.0,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-152",
    "Epic": "Consolidated Dashboard",
    "Tool_Process": "Output Generation",
    "Feature": "Security & access control",
    "User_Story": "As a user I want role-based access to dashboard data\nSo that sensitive information is protected",
    "Description": null,
    "Scenario": "\nScenario: Enforce role-based access and data scoping\n  Given user role and BU entitlements\n  When the user opens the dashboard\n  Then the data must be scoped to authorized BUs/categories/suppliers\n  And exports must honor the same scoping\n",
    "Acceptance_Criteria": "1. RBAC applied to API queries and UI.\n2. Audit user actions: filter changes, exports, drill-downs (timestamp + user ID).\n3. Mask sensitive supplier attributes where policy requires.",
    "Sprint": null,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-153",
    "Epic": "Consolidated Dashboard",
    "Tool_Process": "Output Generation",
    "Feature": "Generate CSV File for PowerBI Visualization",
    "User_Story": "As a user, I want to generate a structured CSV file\nSo that it supports PowerBI visualization and further analysis with minimal manipulation",
    "Description": null,
    "Scenario": "\nScenario: Generate structured CSV for PowerBI\n  Given the processed and cleansed spend data is available\n  When the system generates the output file\n  Then the CSV should include standardized columns:\n    | Supplier Name | Category | Subcategory | BU | Spend Amount | Currency | Contract Compliance | Date |\n  And ensure column headers match PowerBI template requirements\n  And export the file in UTF-8 encoding\n",
    "Acceptance_Criteria": "1. Columns must follow PowerBI schema (predefined template).\n2. Include all mapped fields (supplier, category hierarchy, BU, spend, compliance flags).\n3. Encoding: UTF-8 without BOM.\n4. No merged cells or hidden characters.\n5. Validate row count matches input dataset.\n6. File size optimized for faster download (≤100MB per file split if needed).\n7. Provide timestamp and version in filename (e.g., SpendData_YYYYMMDD_v1.csv).",
    "Sprint": 7.0,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-154",
    "Epic": "Consolidated Dashboard",
    "Tool_Process": "Output Generation",
    "Feature": "Generate CSV File for PowerBI Visualization",
    "User_Story": "As a  User I want the CSV to require minimal manual changes\nSo that it can be uploaded directly to PowerBI dashboards",
    "Description": null,
    "Scenario": "\nScenario: Validate CSV structure for direct PowerBI upload\n  Given the CSV file is generated\n  When the user opens the file\n  Then the column names and data types should match PowerBI schema\n  And numeric fields should be formatted as numbers without currency symbols\n  And date fields should be in YYYY-MM-DD format\n",
    "Acceptance_Criteria": "1. Numeric fields: no commas, no currency symbols.\n2. Date fields: ISO format (YYYY-MM-DD).\n3. No blank headers or duplicate column names.\n4. Validate schema before export; block if mismatched.",
    "Sprint": 7.0,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-155",
    "Epic": "Consolidated Dashboard",
    "Tool_Process": "Output Generation",
    "Feature": "Generate Excel File with Lever Analysis and Contract Data",
    "User_Story": "As a  User I want an Excel file that includes lever analysis pages and contract details\nSo that I can review and share final outputs easily",
    "Description": null,
    "Scenario": "\nScenario: Generate Excel file with lever analysis and contract data\n  Given lever analysis and contract data are finalized\n  When the system generates the Excel file\n  Then the file should include:\n    | Lever Analysis Sheet |\n    | Contract Details Sheet |\n  And apply consistent formatting and headers\n",
    "Acceptance_Criteria": "Sheets:\n\nLever Analysis: Outsource, Automate, Shift levers with impacted HC and cost savings.\nContract Details: Supplier name, contract ID, start/end dates, compliance status.\n\n\nApply conditional formatting for compliance flags.\nInclude summary tables and charts as per template model.",
    "Sprint": 7.0,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-156",
    "Epic": "Consolidated Dashboard",
    "Tool_Process": "Output Generation",
    "Feature": "Generate Excel File with Lever Analysis and Contract Data",
    "User_Story": "As a  User I want embedded formulas in the lever analysis sheet\nSo that calculations are transparent and auditable",
    "Description": null,
    "Scenario": "\nScenario: Embed formulas for lever calculations\n  Given lever analysis metrics are computed\n  When the system generates the Excel file\n  Then formulas should be embedded for:\n    | Impacted HC Calculation |\n    | Remaining HC Calculation |\n    | Cost Savings Calculation |\n  And formulas should reference visible cells (not hidden)\n",
    "Acceptance_Criteria": "Formulas must match logic from diligence calculator template.\nNo hard-coded values; all calculations dynamic.\nProtect formula cells from accidental edits but allow user to view them.",
    "Sprint": 7.0,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-157",
    "Epic": "Consolidated Dashboard",
    "Tool_Process": "Output Generation",
    "Feature": "Generate Excel File with Lever Analysis and Contract Data",
    "User_Story": "As a User I want instructions for merging spend data manually\nSo that I can complete the final lever analysis",
    "Description": null,
    "Scenario": "\nScenario: Provide instructions for manual spend data merge\n  Given the Excel file is generated\n  When the user opens the file\n  Then the system should include a “ReadMe” sheet with steps:\n    | Copy spend data into Spend tab |\n    | Validate row count |\n    | Refresh pivot tables |\n",
    "Acceptance_Criteria": "Include clear instructions in a dedicated sheet.\nHighlight mandatory fields for spend tab.\nProvide validation checks (row count, column alignment).",
    "Sprint": 7.0,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  },
  {
    "ID": "US-158",
    "Epic": "Generate summary output",
    "Tool_Process": "Output Generation",
    "Feature": "Consolidated Savings Summary",
    "User_Story": "As a user I want a consolidated summary output with savings initiatives categorized by type\nSo that stakeholders can see baseline and savings ranges (low/high) across categories",
    "Description": null,
    "Scenario": "\nScenario: Produce consolidated summary by category\n  Given baseline spend is available for HC, Non-HC, and Real Estate\n  And savings percentages (low and high) are defined for each category\n  When the system generates the consolidated summary\n  Then the output should include rows per category with columns:\n    | Category | Baseline Spend | Low Savings % | High Savings % | Low Savings $ | High Savings $ |\n  And display a subtotal per category group and a grand total row\n",
    "Acceptance_Criteria": "Required categories: HC, Non-HC, Real Estate (exact labels).\nColumns present: Category, Baseline Spend, Low Savings %, High Savings %, Low Savings $, High Savings $.\nLow Savings $ = Baseline Spend × Low Savings %.\nHigh Savings $ = Baseline Spend × High Savings %.\nSubtotals: Sum of baseline and savings (low/high) per category group if the category contains sub-rows.\nGrand Total: Sum across all categories for Baseline, Low $, and High $.\n\n",
    "Sprint": 7.0,
    "Effort_Point": null,
    "Effort_Required": null,
    "Dependencies": null,
    "Priority": null,
    "Parent": null,
    "Date_Received": null,
    "Received_By": null,
    "Version_Assignment": null,
    "PPMD_Approver": null,
    "Status": null,
    "Comments": null
  }
]